{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mVEArGxNd_sU",
    "outputId": "185d1ef7-67ae-408f-eef3-33b6e16a8bd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://data.pyg.org/whl/torch-2.9.0+cu126.html\n",
      "Collecting torch-scatter\n",
      "  Downloading torch_scatter-2.1.2.tar.gz (108 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Building wheels for collected packages: torch-scatter\n",
      "  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for torch-scatter: filename=torch_scatter-2.1.2-cp312-cp312-linux_x86_64.whl size=664339 sha256=9f3fc469fdcec0327dbe5c356e25c8ba431ecf28e185fc9d7438d80fa8af021b\n",
      "  Stored in directory: /root/.cache/pip/wheels/84/20/50/44800723f57cd798630e77b3ec83bc80bd26a1e3dc3a672ef5\n",
      "Successfully built torch-scatter\n",
      "Installing collected packages: torch-scatter\n",
      "Successfully installed torch-scatter-2.1.2\n",
      "Collecting torch-geometric\n",
      "  Downloading torch_geometric-2.7.0-py3-none-any.whl.metadata (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.13.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2025.3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.1.6)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.0.2)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (5.9.5)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.2.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.32.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.22.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch-geometric) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2025.11.12)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch-geometric) (4.15.0)\n",
      "Downloading torch_geometric-2.7.0-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch-geometric\n",
      "Successfully installed torch-geometric-2.7.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "TORCH = torch.__version__.split('+')[0]\n",
    "CUDA = 'cu' + torch.version.cuda.replace('.', '')\n",
    "\n",
    "# 2. Install torch-scatter, torch-sparse, and finally, torch-geometric\n",
    "!pip install torch-scatter -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
    "!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TsRa1G6rdSIM",
    "outputId": "e9eb103b-2633-461b-b141-350ca82bdce8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes: 2708\n",
      "Features: 1433\n",
      "Classes: 7\n",
      "\n",
      "Total number of directed edges: 10556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "dataset = Planetoid(root='./data/Cora', name='Cora')\n",
    "data = dataset[0]\n",
    "G = to_networkx(data, to_undirected=True)\n",
    "\n",
    "print(f\"Nodes: {data.num_nodes}\")\n",
    "print(f\"Features: {data.num_node_features}\")\n",
    "print(f\"Classes: {dataset.num_classes}\")\n",
    "\n",
    "edge_index = data.edge_index\n",
    "\n",
    "\n",
    "num_edges_to_print = min(5, edge_index.shape[1])\n",
    "\n",
    "# Print the total number of edges\n",
    "print(f\"\\nTotal number of directed edges: {data.num_edges}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q7EmqHrapGyy",
    "outputId": "d92b16d8-9c89-4b0d-f3cb-a850b2777181"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Node Properties:\n",
      "         degree_centrality  closeness_centrality  betweenness_centrality\n",
      "node_id                                                                 \n",
      "0                 0.001108              0.144255            9.766154e-07\n",
      "1                 0.001108              0.151453            1.080477e-03\n",
      "2                 0.001847              0.179168            4.050816e-03\n",
      "3                 0.000369              0.000369            0.000000e+00\n",
      "4                 0.001847              0.153266            5.511762e-04\n"
     ]
    }
   ],
   "source": [
    "\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "\n",
    "closeness_centrality = nx.closeness_centrality(G)\n",
    "\n",
    "betweenness_centrality = nx.betweenness_centrality(G)\n",
    "\n",
    "node_properties_df = pd.DataFrame({\n",
    "    'node_id': list(G.nodes()),\n",
    "    'degree_centrality': [degree_centrality[n] for n in G.nodes()],\n",
    "    'closeness_centrality': [closeness_centrality[n] for n in G.nodes()],\n",
    "    'betweenness_centrality': [betweenness_centrality[n] for n in G.nodes()]\n",
    "}).set_index('node_id')\n",
    "\n",
    "print(\"Sample Node Properties:\")\n",
    "print(node_properties_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UiDHQYGxzKwI",
    "outputId": "1df884fb-b5a8-4af5-c388-f3ec6d88283c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Node Properties with Ranks:\n",
      "         degree_centrality  degree_centrality_rank\n",
      "node_id                                           \n",
      "0                 0.001108               49.667651\n",
      "1                 0.001108               49.667651\n",
      "2                 0.001847               79.431315\n",
      "3                 0.000369                8.973412\n",
      "4                 0.001847               79.431315\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Calculate and Add Rank to DataFrame ---\n",
    "\n",
    "# The TANS method provides the rank (percentile) of each property\n",
    "# among all nodes to the LLM (Table 2 in the paper).\n",
    "for col in node_properties_df.columns:\n",
    "    # Use .rank(pct=True) to calculate percentile rank (0 to 1)\n",
    "    # Multiply by 100 to get the percentage rank (0 to 100)\n",
    "    node_properties_df[f'{col}_rank'] = node_properties_df[col].rank(pct=True) * 100\n",
    "\n",
    "print(\"\\nSample Node Properties with Ranks:\")\n",
    "print(node_properties_df[['degree_centrality', 'degree_centrality_rank']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1Jvf6HURzNaa"
   },
   "outputs": [],
   "source": [
    "# --- 2. Helper Functions for Text Retrieval ---\n",
    "\n",
    "def get_original_text(node_id):\n",
    "    \"\"\"\n",
    "    Placeholder: In a full TANS implementation, this reads the actual title\n",
    "    and abstract text from the raw Cora dataset files.\n",
    "    \"\"\"\n",
    "    # Using a generic placeholder for demonstration\n",
    "    return \"A paper discussing graph convolutional networks and deep learning for node classification. This is the abstract content.\"\n",
    "\n",
    "\n",
    "def get_neighbor_texts(graph, node_id, num_neighbors=5):\n",
    "    \"\"\"\n",
    "    Placeholder: Fetches the text (e.g., titles/abstracts) of k=5 neighboring nodes.\n",
    "\n",
    "    The paper specifies randomly selecting k=5 neighbors to provide additional context\n",
    "    (Prompt 3: Optional Neighbor Text).\n",
    "    \"\"\"\n",
    "    neighbors = list(graph.neighbors(node_id))\n",
    "\n",
    "    if not neighbors:\n",
    "        return \"No connected nodes found.\"\n",
    "\n",
    "    # Randomly select up to num_neighbors texts\n",
    "    selected_neighbors = np.random.choice(neighbors, min(len(neighbors), num_neighbors), replace=False)\n",
    "\n",
    "    neighbor_descriptions = []\n",
    "    for n in selected_neighbors:\n",
    "        # Recursively call the original text function for the neighbor\n",
    "        neighbor_descriptions.append(f\"Node {n}: {get_original_text(n)[:40]}...\")\n",
    "\n",
    "    return \"\\n\".join(neighbor_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-yS0SiSrzU9Z"
   },
   "outputs": [],
   "source": [
    "# --- 3. Final TANS Prompt Generation Function (Steps 2 & 3) ---\n",
    "\n",
    "def generate_tans_prompt(graph, node_id, properties_df, classes):\n",
    "    \"\"\"\n",
    "    Generates the complete, structured TANS prompt using all information.\n",
    "    \"\"\"\n",
    "    # Retrieve properties and ranks\n",
    "    degree = properties_df.loc[node_id, 'degree_centrality']\n",
    "    rank_degree = properties_df.loc[node_id, 'degree_centrality_rank']\n",
    "\n",
    "    # 1. Get original text (Prompt 2)\n",
    "    original_text = get_original_text(node_id)\n",
    "\n",
    "    # 2. Get neighbor texts (Prompt 3)\n",
    "    neighbor_texts = get_neighbor_texts(graph, node_id, num_neighbors=5)\n",
    "\n",
    "    # 3. Construct the full prompt (Prefix, Text, Neighbor, Property, Suffix)\n",
    "    prompt = f\"\"\"\n",
    "Given a node from a citation network graph, where the node type is paper.\n",
    "The original node description is: \"{original_text}\".\n",
    "\n",
    "The following are the textual information of 5 connected nodes. The descriptions are:\n",
    "{neighbor_texts}\n",
    "\n",
    "Node Properties:\n",
    "- Degree Centrality value: {degree:.4f}, ranked as {rank_degree:.2f}% among all nodes.\n",
    "- Closeness Centrality value: {properties_df.loc[node_id, 'closeness_centrality']:.4f}.\n",
    "- Betweenness Centrality value: {properties_df.loc[node_id, 'betweenness_centrality']:.4f}.\n",
    "\n",
    "Output the potential class of the node among the following classes: {classes}.\n",
    "Provide reasons for your assessment. Your answer should be less than 200 words.\n",
    "\"\"\"\n",
    "    return prompt.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8jCHPh1vzWTr",
    "outputId": "bb65889f-6996-414a-9ae9-842a7740aaae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Generated TANS Prompt Example ---\n",
      "Given a node from a citation network graph, where the node type is paper.\n",
      "The original node description is: \"A paper discussing graph convolutional networks and deep learning for node classification. This is the abstract content.\".\n",
      "\n",
      "The following are the textual information of 5 connected nodes. The descriptions are:\n",
      "Node 1602: A paper discussing graph convolutional n...\n",
      "Node 2056: A paper discussing graph convolutional n...\n",
      "\n",
      "Node Properties:\n",
      "- Degree Centrality value: 0.0007, ranked as 28.69% among all nodes.\n",
      "- Closeness Centrality value: 0.1419.\n",
      "- Betweenness Centrality value: 0.0001.\n",
      "\n",
      "Output the potential class of the node among the following classes: ['Neural Networks', 'Probabilistic Methods', 'Genetic Algorithms', 'Theory', 'Case Based', 'Reinforcement Learning', 'Rule Learning']. \n",
      "Provide reasons for your assessment. Your answer should be less than 200 words.\n"
     ]
    }
   ],
   "source": [
    "# Example call with the required arguments\n",
    "classes_cora = [\"Neural Networks\", \"Probabilistic Methods\", \"Genetic Algorithms\", \"Theory\", \"Case Based\", \"Reinforcement Learning\", \"Rule Learning\"]\n",
    "sample_node_id = list(G.nodes())[100] # Assuming G is defined from your previous steps\n",
    "\n",
    "final_prompt = generate_tans_prompt(\n",
    "    G, # Pass the NetworkX graph\n",
    "    sample_node_id,\n",
    "    node_properties_df,\n",
    "    classes_cora\n",
    ")\n",
    "print(\"\\n--- Final Generated TANS Prompt Example ---\")\n",
    "print(final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oXwJIj2qu22e",
    "outputId": "a9b01c51-c526-4b9c-b447-ac1692518100"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini-Generated TANS Description:\n",
      "The potential class of the node is **Neural Networks**.\n",
      "\n",
      "**Reasons:**\n",
      "\n",
      "1.  **Node Description:** The paper explicitly discusses \"graph convolutional networks\" and \"deep learning.\" Graph Convolutional Networks (GCNs) are a prominent type of neural network designed for graph-structured data, and \"deep learning\" is the core paradigm within the field of neural networks.\n",
      "2.  **Connected Nodes:** The descriptions of connected nodes (Node 1602, Node 2056) also mention \"graph convolutional n...\", reinforcing the focus on this specific neural network architecture.\n",
      "3.  **Class Fit:** Among the given options, \"Neural Networks\" is the direct and most accurate classification for content involving \"graph convolutional networks\" and \"deep learning.\" The other classes (Probabilistic Methods, Genetic Algorithms, etc.) do not align with these specific technical terms.\n",
      "\n",
      "The centrality measures describe the node's position in the network but do not provide direct evidence for its content classification in this context.\n"
     ]
    }
   ],
   "source": [
    "# Install the necessary library (if not already done)\n",
    "# pip install google-genai\n",
    "\n",
    "import os\n",
    "from google import genai\n",
    "from google.genai.errors import APIError\n",
    "\n",
    "# --- IMPORTANT: Set your API Key ---\n",
    "# It's best practice to load your API key from an environment variable.\n",
    "os.environ['GEMINI_API_KEY'] = 'YOUR API KEY HERE'\n",
    "client = genai.Client()\n",
    "# Assuming the client is initialized globally or passed in\n",
    "def query_llm_and_generate_description_gemini(prompt):\n",
    "    \"\"\"\n",
    "    Calls the Gemini API with the TANS prompt and returns the generated text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize client inside if not done globally\n",
    "        client = genai.Client()\n",
    "\n",
    "        # Call the Gemini API\n",
    "        response = client.models.generate_content(\n",
    "            model='gemini-2.5-flash', # Use a capable model like flash or pro\n",
    "            contents=prompt\n",
    "        )\n",
    "\n",
    "        # The TANS explanation is the generated text\n",
    "        llm_explanation = response.text\n",
    "        return llm_explanation\n",
    "\n",
    "    except APIError as e:\n",
    "        print(f\"Gemini API Error: {e}\")\n",
    "        return \"Error: Could not generate description due to API error.\"\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return \"Error: An unexpected error occurred.\"\n",
    "\n",
    "# Example Usage (replace the placeholder call):\n",
    "llm_generated_text = query_llm_and_generate_description_gemini(final_prompt)\n",
    "print(f\"Gemini-Generated TANS Description:\\n{llm_generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-gJcqKsE50zq",
    "outputId": "eb6f245b-c884-42d8-c9ff-9d5dd14c0a78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Pubmed Dataset ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.test.index\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes: 19717, Original Features: 500, Classes: 3\n",
      "Pubmed setup complete.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the Pubmed dataset\n",
    "print(\"--- Loading Pubmed Dataset ---\")\n",
    "dataset = Planetoid(root='./data/Pubmed', name='Pubmed')\n",
    "data = dataset[0]\n",
    "print(f\"Nodes: {data.num_nodes}, Original Features: {data.num_node_features}, Classes: {dataset.num_classes}\")\n",
    "\n",
    "# Convert to NetworkX\n",
    "G = to_networkx(data, to_undirected=True)\n",
    "\n",
    "# Calculate Centralities (Required TANS Step 1)\n",
    "# Note: This is a slow step (especially Betweenness) and is conceptually similar to Cora.\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "closeness_centrality = nx.closeness_centrality(G)\n",
    "betweenness_centrality = nx.betweenness_centrality(G)\n",
    "\n",
    "# Create DataFrame\n",
    "node_properties_df = pd.DataFrame({\n",
    "    'node_id': list(G.nodes()),\n",
    "    'degree_centrality': [degree_centrality[n] for n in G.nodes()],\n",
    "    'closeness_centrality': [closeness_centrality[n] for n in G.nodes()],\n",
    "    'betweenness_centrality': [betweenness_centrality[n] for n in G.nodes()]\n",
    "}).set_index('node_id')\n",
    "\n",
    "# Calculate Ranks (Required TANS Step 2 for Prompting)\n",
    "for col in node_properties_df.columns:\n",
    "    node_properties_df[f'{col}_rank'] = node_properties_df[col].rank(pct=True) * 100\n",
    "\n",
    "# --- Global Variables for Pubmed Prompting ---\n",
    "# Pubmed has 3 classes (e.g., specific types of diabetic papers)\n",
    "classes_pubmed = [\"Experimental Diabetes\", \"Diabetes Mellitus\", \"Type 1 Diabetes\"] # Placeholder for actual class names\n",
    "sample_node_id = list(G.nodes())[500]\n",
    "sample_text = \"A paper discussing a novel finding related to insulin resistance in mice.\"\n",
    "\n",
    "print(\"Pubmed setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "IwJ_ItpAT5ew"
   },
   "outputs": [],
   "source": [
    "# --- USA Airport Network (Text-Free) Setup ---\n",
    "\n",
    "# This dataset is not part of Planetoid. You would need to load it from a source\n",
    "# like the official TANS repository or the original data source (e.g., using NetworkX\n",
    "# after downloading the edge list).\n",
    "# --- Conceptual Loading ---\n",
    "# G_usa = nx.read_edgelist('usa.edgelist', nodetype=int)\n",
    "\n",
    "# --- MOCKING DATA FOR CONTINUITY ---\n",
    "# Since direct loading is complex, we mock a small text-free graph for demonstration.\n",
    "G_usa_mock = nx.random_geometric_graph(n=1190, radius=0.1) # Mock USA graph (1,190 nodes)\n",
    "\n",
    "# Calculate Centralities\n",
    "degree_centrality_usa = nx.degree_centrality(G_usa_mock)\n",
    "# ... (rest of centrality calculations)\n",
    "\n",
    "# --- Global Variables for USA Prompting ---\n",
    "# Airport classes relate to activity level\n",
    "classes_usa = [\"High Activity\", \"Moderate Activity\", \"Moderately Low Activity\", \"Low Activity\"] # 4 classes\n",
    "sample_node_id_usa = list(G_usa_mock.nodes())[50]\n",
    "sample_text_usa = \"\" # CRITICAL: Text-free means the original text is empty.\n",
    "\n",
    "# When generating the prompt for text-free graphs,\n",
    "# you use the 'generate_tans_prompt' function but pass an empty string\n",
    "# for 'original_text' and the 'get_neighbor_texts' function should be adapted\n",
    "# to return \"No textual descriptions available\" for its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X-3gZFoGT79H",
    "outputId": "b07cec1c-0e5f-42c9-d72e-836831d5a37c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Generated TANS Prompt Example (USA - Text-Free) ---\n",
      "Given a node from a citation network graph, where the node type is paper.\n",
      "The original node description is: \"A paper discussing graph convolutional networks and deep learning for node classification. This is the abstract content.\".\n",
      "\n",
      "The following are the textual information of 5 connected nodes. The descriptions are:\n",
      "Node 231: A paper discussing graph convolutional n...\n",
      "Node 128: A paper discussing graph convolutional n...\n",
      "Node 427: A paper discussing graph convolutional n...\n",
      "Node 205: A paper discussing graph convolutional n...\n",
      "Node 24: A paper discussing graph convolutional n...\n",
      "\n",
      "Node Properties:\n",
      "- Degree Centrality value: 0.1749, ranked as 99.58% among all nodes.\n",
      "- Closeness Centrality value: 0.1458.\n",
      "- Betweenness Centrality value: 0.0000.\n",
      "\n",
      "Output the potential class of the node among the following classes: ['High Activity', 'Moderate Activity', 'Moderately Low Activity', 'Low Activity']. \n",
      "Provide reasons for your assessment. Your answer should be less than 200 words.\n"
     ]
    }
   ],
   "source": [
    "# --- Re-using the prompt logic with Text-Free adaptation ---\n",
    "\n",
    "def get_original_text_text_free(node_id):\n",
    "    \"\"\"Returns empty string for text-free graphs.\"\"\"\n",
    "    return \"\"\n",
    "\n",
    "def get_neighbor_texts_text_free(graph, node_id, num_neighbors=5):\n",
    "    \"\"\"Returns a placeholder indicating no neighbor text exists.\"\"\"\n",
    "    return \"No textual descriptions available for connected nodes.\"\n",
    "\n",
    "\n",
    "# --- Example Prompt for a Text-Free Graph (USA) ---\n",
    "\n",
    "# 1. Extract node properties as a *DataFrame*, not a Series\n",
    "high_activity_props = node_properties_df.loc[[sample_node_id_usa]].copy()\n",
    "\n",
    "# 2. Add new values safely\n",
    "high_activity_props.loc[sample_node_id_usa, \"degree_centrality\"] = 0.1749\n",
    "high_activity_props.loc[sample_node_id_usa, \"degree_centrality_rank\"] = 99.58  # mock rank\n",
    "\n",
    "# 3. Now generate the prompt using the retained 2D structure\n",
    "final_prompt_usa = generate_tans_prompt(\n",
    "    G_usa_mock,\n",
    "    sample_node_id_usa,\n",
    "    high_activity_props,   # <-- now a DataFrame\n",
    "    classes_usa\n",
    ")\n",
    "\n",
    "print(\"\\n--- Final Generated TANS Prompt Example (USA - Text-Free) ---\")\n",
    "print(final_prompt_usa)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
