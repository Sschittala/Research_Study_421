{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "TORCH = torch.__version__.split('+')[0]\n",
        "CUDA = 'cu' + torch.version.cuda.replace('.', '')\n",
        "\n",
        "# 2. Install torch-scatter, torch-sparse, and finally, torch-geometric\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVEArGxNd_sU",
        "outputId": "460eb47d-25a6-48f7-86cd-08453f34734c"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-2.9.0+cu126.html\n",
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.12/dist-packages (2.1.2)\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.12/dist-packages (2.7.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.13.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch-geometric) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2025.11.12)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch-geometric) (4.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsRa1G6rdSIM",
        "outputId": "bfaeebbd-4ff4-4435-9629-c8dc9b8400f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nodes: 2708\n",
            "Features: 1433\n",
            "Classes: 7\n",
            "\n",
            "Total number of directed edges: 10556\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch_geometric.datasets import Planetoid\n",
        "import networkx as nx\n",
        "from torch_geometric.utils import to_networkx\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "dataset = Planetoid(root='./data/Cora', name='Cora')\n",
        "data = dataset[0]\n",
        "G = to_networkx(data, to_undirected=True)\n",
        "\n",
        "print(f\"Nodes: {data.num_nodes}\")\n",
        "print(f\"Features: {data.num_node_features}\")\n",
        "print(f\"Classes: {dataset.num_classes}\")\n",
        "\n",
        "edge_index = data.edge_index\n",
        "\n",
        "\n",
        "num_edges_to_print = min(5, edge_index.shape[1])\n",
        "\n",
        "# Print the total number of edges\n",
        "print(f\"\\nTotal number of directed edges: {data.num_edges}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "degree_centrality = nx.degree_centrality(G)\n",
        "\n",
        "closeness_centrality = nx.closeness_centrality(G)\n",
        "\n",
        "betweenness_centrality = nx.betweenness_centrality(G)\n",
        "\n",
        "node_properties_df = pd.DataFrame({\n",
        "    'node_id': list(G.nodes()),\n",
        "    'degree_centrality': [degree_centrality[n] for n in G.nodes()],\n",
        "    'closeness_centrality': [closeness_centrality[n] for n in G.nodes()],\n",
        "    'betweenness_centrality': [betweenness_centrality[n] for n in G.nodes()]\n",
        "}).set_index('node_id')\n",
        "\n",
        "print(\"Sample Node Properties:\")\n",
        "print(node_properties_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7EmqHrapGyy",
        "outputId": "174a593f-040f-4ce2-a575-86c9e6a26ae2"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Node Properties:\n",
            "         degree_centrality  closeness_centrality  betweenness_centrality\n",
            "node_id                                                                 \n",
            "0                 0.001108              0.144255            9.766154e-07\n",
            "1                 0.001108              0.151453            1.080477e-03\n",
            "2                 0.001847              0.179168            4.050816e-03\n",
            "3                 0.000369              0.000369            0.000000e+00\n",
            "4                 0.001847              0.153266            5.511762e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Calculate and Add Rank to DataFrame ---\n",
        "\n",
        "# The TANS method provides the rank (percentile) of each property\n",
        "# among all nodes to the LLM (Table 2 in the paper).\n",
        "for col in node_properties_df.columns:\n",
        "    # Use .rank(pct=True) to calculate percentile rank (0 to 1)\n",
        "    # Multiply by 100 to get the percentage rank (0 to 100)\n",
        "    node_properties_df[f'{col}_rank'] = node_properties_df[col].rank(pct=True) * 100\n",
        "\n",
        "print(\"\\nSample Node Properties with Ranks:\")\n",
        "print(node_properties_df[['degree_centrality', 'degree_centrality_rank']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiDHQYGxzKwI",
        "outputId": "eb44d556-698b-4936-dfa5-856c9c3cc3f3"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample Node Properties with Ranks:\n",
            "         degree_centrality  degree_centrality_rank\n",
            "node_id                                           \n",
            "0                 0.001108               49.667651\n",
            "1                 0.001108               49.667651\n",
            "2                 0.001847               79.431315\n",
            "3                 0.000369                8.973412\n",
            "4                 0.001847               79.431315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "\n",
        "# Load node data\n",
        "cora_content = pd.read_csv(\"/content/cora/cora.content\", sep=\"\\t\", header=None)\n",
        "\n",
        "# Last column is the class label\n",
        "cora_content.columns = [\"id\"] + [f\"w{i}\" for i in range(1, cora_content.shape[1]-1)] + [\"class\"]\n",
        "\n",
        "# Load edge list (citations)\n",
        "edges = pd.read_csv(\"/content/cora/cora.cites\", sep=\"\\t\", header=None, names=[\"source\", \"target\"])\n",
        "\n",
        "# Build directed citation graph\n",
        "G = nx.from_pandas_edgelist(edges, source=\"source\", target=\"target\", create_using=nx.DiGraph())\n",
        "\n",
        "def bow_to_text(row):\n",
        "    words = [col for col in row.index if col.startswith(\"w\") and row[col] == 1]\n",
        "    return \" \".join(words)\n",
        "\n",
        "node_text = {\n",
        "    row[\"id\"]: bow_to_text(row)  # 'id' must match G node IDs\n",
        "    for _, row in cora_content.iterrows()\n",
        "}\n",
        "\n",
        "def get_original_text(node_id):\n",
        "    \"\"\"\n",
        "    Placeholder: In a full TANS implementation, this reads the actual title\n",
        "    and abstract text from the raw Cora dataset files.\n",
        "    \"\"\"\n",
        "    # Using a generic placeholder for demonstration\n",
        "    if node_id not in node_text:\n",
        "      return \"No text found for this node.\"\n",
        "    return node_text[node_id]"
      ],
      "metadata": {
        "id": "1Jvf6HURzNaa"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_nodes = list(set(G.nodes()).intersection(node_text.keys()))\n",
        "sample_node_id = common_nodes[0]"
      ],
      "metadata": {
        "id": "DDbo_tsuZKiK"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_neighbor_texts(graph, node_id, num_neighbors=5):\n",
        "    \"\"\"\n",
        "    Retrieves up to num_neighbors real texts of connected nodes.\n",
        "    \"\"\"\n",
        "    if node_id not in graph:\n",
        "        return \"Node not found in graph.\"\n",
        "\n",
        "    neighbors = list(graph.neighbors(node_id))\n",
        "\n",
        "    if len(neighbors) == 0:\n",
        "        return \"No connected nodes found.\"\n",
        "\n",
        "    selected = np.random.choice(\n",
        "        neighbors,\n",
        "        min(num_neighbors, len(neighbors)),\n",
        "        replace=False\n",
        "    )\n",
        "\n",
        "    return \"\\n\".join(\n",
        "        [\n",
        "            f\"{n}: {get_original_text(n)[:200]}...\"\n",
        "            for n in selected\n",
        "        ]\n",
        "    )\n",
        "test_node = cora_content.iloc[0][\"id\"]\n",
        "\n",
        "print(\"--- Original Text ---\")\n",
        "print(get_original_text(test_node))\n",
        "\n",
        "print(\"\\n--- Neighbor Texts ---\")\n",
        "print(get_neighbor_texts(G, test_node, num_neighbors=5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xl_60SlSVvsf",
        "outputId": "1cf93258-65af-47ef-e128-eb86d498c196"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Original Text ---\n",
            "w119 w126 w177 w253 w352 w457 w508 w522 w620 w649 w699 w703 w735 w846 w903 w1206 w1210 w1237 w1353 w1427\n",
            "\n",
            "--- Neighbor Texts ---\n",
            "686532: w133 w174 w212 w329 w330 w336 w435 w522 w565 w704 w726 w730 w798 w1171 w1209 w1212 w1258 w1302 w1329 w1340 w1424 w1426...\n",
            "31349: w457 w649 w903 w1210 w1274...\n",
            "1129442: w133 w136 w232 w238 w251 w265 w331 w469 w699 w875 w903 w1020 w1098 w1136 w1274 w1349 w1353 w1360...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. Final TANS Prompt Generation Function (Steps 2 & 3) ---\n",
        "\n",
        "def generate_tans_prompt(graph, node_id, properties_df, classes):\n",
        "    \"\"\"\n",
        "    Generates the complete, structured TANS prompt using all information.\n",
        "    \"\"\"\n",
        "    # Retrieve properties and ranks\n",
        "    degree = properties_df.loc[node_id, 'degree_centrality']\n",
        "    rank_degree = properties_df.loc[node_id, 'degree_centrality_rank']\n",
        "\n",
        "    # 1. Get original text (Prompt 2)\n",
        "    original_text = get_original_text(node_id)\n",
        "\n",
        "    # 2. Get neighbor texts (Prompt 3)\n",
        "    neighbor_texts = get_neighbor_texts(graph, node_id, num_neighbors=5)\n",
        "\n",
        "    # 3. Construct the full prompt (Prefix, Text, Neighbor, Property, Suffix)\n",
        "    prompt = f\"\"\"\n",
        "Given a node from a citation network graph, where the node type is paper.\n",
        "The original node description is: \"{original_text}\".\n",
        "\n",
        "The following are the textual information of 5 connected nodes. The descriptions are:\n",
        "{neighbor_texts}\n",
        "\n",
        "Node Properties:\n",
        "- Degree Centrality value: {degree:.4f}, ranked as {rank_degree:.2f}% among all nodes.\n",
        "- Closeness Centrality value: {properties_df.loc[node_id, 'closeness_centrality']:.4f}.\n",
        "- Betweenness Centrality value: {properties_df.loc[node_id, 'betweenness_centrality']:.4f}.\n",
        "\n",
        "Output the potential class of the node among the following classes: {classes}.\n",
        "Provide reasons for your assessment. Your answer should be less than 200 words.\n",
        "\"\"\"\n",
        "    return prompt.strip()"
      ],
      "metadata": {
        "id": "-yS0SiSrzU9Z"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure node_properties_df is indexed by node_id\n",
        "\n",
        "# Find nodes common to G, node_text, and node_properties_df\n",
        "common_nodes = list(\n",
        "    set(G.nodes()).intersection(node_text.keys()).intersection(node_properties_df.index)\n",
        ")\n",
        "\n",
        "# Pick a sample node\n",
        "sample_node_id = common_nodes[0]  # safe\n"
      ],
      "metadata": {
        "id": "XeaVuw3La1iC"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sample node ID:\", sample_node_id)\n",
        "print(\"In G:\", sample_node_id in G)\n",
        "print(\"In node_text:\", sample_node_id in node_text)\n",
        "print(\"In node_properties_df:\", sample_node_id in node_properties_df.index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7f0Rcr_arkM",
        "outputId": "866f4c58-3e8e-463e-a416-e43aa28a7807"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample node ID: 128\n",
            "In G: True\n",
            "In node_text: True\n",
            "In node_properties_df: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "classes_cora = [\"Neural Networks\", \"Probabilistic Methods\", \"Genetic Algorithms\", \"Theory\", \"Case Based\", \"Reinforcement Learning\", \"Rule Learning\"]\n",
        "\n",
        "\n",
        "common_nodes = list(\n",
        "    set(G.nodes()).intersection(node_text.keys()).intersection(node_properties_df.index)\n",
        ")\n",
        "\n",
        "# Pick a sample node\n",
        "sample_node_id = common_nodes[0]\n",
        "\n",
        "# Generate TANS prompt\n",
        "final_prompt = generate_tans_prompt(\n",
        "    G,\n",
        "    sample_node_id,\n",
        "    node_properties_df,\n",
        "    classes_cora\n",
        ")\n",
        "print(\"\\n--- Final Generated TANS Prompt Example ---\")\n",
        "print(final_prompt)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jCHPh1vzWTr",
        "outputId": "4a7cec05-0500-49f3-db94-0b4386e06f7c"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Final Generated TANS Prompt Example ---\n",
            "Given a node from a citation network graph, where the node type is paper.\n",
            "The original node description is: \"w2 w42 w188 w213 w358 w405 w465 w506 w508 w582 w636 w875 w989 w1072 w1231 w1232 w1259 w1264 w1275 w1394\".\n",
            "\n",
            "The following are the textual information of 5 connected nodes. The descriptions are:\n",
            "20526: w100 w241 w331 w335 w549 w582 w633 w649 w830 w875 w1072 w1119 w1132 w1156 w1178 w1193 w1207 w1264 w1275 w1360 w1433...\n",
            "91975: w158 w212 w238 w357 w447 w521 w595 w605 w624 w649 w656 w724 w830 w875 w940 w1072 w1264 w1275 w1309 w1360 w1424...\n",
            "1114125: w94 w100 w335 w402 w582 w605 w774 w981 w1156 w1178 w1264 w1293 w1307 w1315 w1321 w1382...\n",
            "39403: w127 w293 w335 w549 w582 w605 w626 w774 w912 w973 w989 w1133 w1156 w1263 w1264 w1293 w1307 w1315 w1321 w1382...\n",
            "\n",
            "Node Properties:\n",
            "- Degree Centrality value: 0.0015, ranked as 67.06% among all nodes.\n",
            "- Closeness Centrality value: 0.1304.\n",
            "- Betweenness Centrality value: 0.0007.\n",
            "\n",
            "Output the potential class of the node among the following classes: ['Neural Networks', 'Probabilistic Methods', 'Genetic Algorithms', 'Theory', 'Case Based', 'Reinforcement Learning', 'Rule Learning']. \n",
            "Provide reasons for your assessment. Your answer should be less than 200 words.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the necessary library (if not already done)\n",
        "# pip install google-genai\n",
        "\n",
        "import os\n",
        "from google import genai\n",
        "from google.genai.errors import APIError\n",
        "\n",
        "# --- IMPORTANT: Set your API Key ---\n",
        "# It's best practice to load your API key from an environment variable.\n",
        "os.environ['GEMINI_API_KEY'] = 'AIzaSyAjX7evzLq__dhbzcJo8uNVkpEyp6JeQQY'\n",
        "client = genai.Client()\n",
        "# Assuming the client is initialized globally or passed in\n",
        "def query_llm_and_generate_description_gemini(prompt, class_list):\n",
        "    \"\"\"\n",
        "    Calls the Gemini API with the TANS prompt and returns the generated text.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Initialize client inside if not done globally\n",
        "        client = genai.Client()\n",
        "\n",
        "        # Call the Gemini API\n",
        "        response = client.models.generate_content(\n",
        "            model='gemini-2.5-flash', # Use a capable model like flash or pro\n",
        "            contents=prompt\n",
        "        )\n",
        "\n",
        "        # The TANS explanation is the generated text\n",
        "        llm_explanation = response.text\n",
        "        predicted_class = None\n",
        "        for cls in class_list:\n",
        "            if cls.lower() in llm_explanation.lower():\n",
        "                predicted_class = cls\n",
        "                break\n",
        "\n",
        "        return predicted_class, llm_explanation\n",
        "\n",
        "    except APIError as e:\n",
        "        print(f\"Gemini API Error: {e}\")\n",
        "        return \"Error: Could not generate description due to API error.\"\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return \"Error: An unexpected error occurred.\"\n",
        "\n",
        "# Example Usage (replace the placeholder call):\n",
        "classes_cora = [\n",
        "    \"Neural Networks\", \"Probabilistic Methods\", \"Genetic Algorithms\",\n",
        "    \"Theory\", \"Case Based\", \"Reinforcement Learning\", \"Rule Learning\"\n",
        "]\n",
        "predicted_class, llm_generated_text = query_llm_and_generate_description_gemini(final_prompt, classes_cora)\n",
        "print(f\"Predicted Class: {predicted_class}\")\n",
        "print(f\"Gemini-Generated TANS Description:\\n{llm_generated_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXwJIj2qu22e",
        "outputId": "99b0cc61-e4f2-43db-ce80-dec02cd3b8d1"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Class: Neural Networks\n",
            "Gemini-Generated TANS Description:\n",
            "The node and its connected papers exhibit a strong overlap in their textual descriptions, with several anonymous keywords appearing frequently across multiple nodes. Most notably, `w1264` is present in all five papers, and `w582` appears in four. Keywords `w875`, `w1072`, and `w1275` are also shared extensively.\n",
            "\n",
            "This high co-occurrence of specific, technical terms suggests a focused research domain with a distinct vocabulary. The node's moderate Degree Centrality (67.06%) indicates it's well-connected within its community, while its low Betweenness Centrality suggests it's not bridging disparate areas. This profile aligns with a paper deeply embedded in a specialized field.\n",
            "\n",
            "Among the given options, **Neural Networks** are characterized by highly specific architectures, algorithms, and components, which often leads to a concentrated and consistently shared technical vocabulary among related papers. This pattern of focused, shared keywords strongly supports a classification in Neural Networks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.datasets import Planetoid\n",
        "import networkx as nx\n",
        "from torch_geometric.utils import to_networkx\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the Pubmed dataset\n",
        "print(\"--- Loading Pubmed Dataset ---\")\n",
        "dataset = Planetoid(root='./data/Pubmed', name='Pubmed')\n",
        "data = dataset[0]\n",
        "print(f\"Nodes: {data.num_nodes}, Original Features: {data.num_node_features}, Classes: {dataset.num_classes}\")\n",
        "\n",
        "# Convert to NetworkX\n",
        "G = to_networkx(data, to_undirected=True)\n",
        "\n",
        "# Calculate Centralities (Required TANS Step 1)\n",
        "# Note: This is a slow step (especially Betweenness) and is conceptually similar to Cora.\n",
        "degree_centrality = nx.degree_centrality(G)\n",
        "closeness_centrality = nx.closeness_centrality(G)\n",
        "betweenness_centrality = nx.betweenness_centrality(G)\n",
        "\n",
        "# Create DataFrame\n",
        "node_properties_df = pd.DataFrame({\n",
        "    'node_id': list(G.nodes()),\n",
        "    'degree_centrality': [degree_centrality[n] for n in G.nodes()],\n",
        "    'closeness_centrality': [closeness_centrality[n] for n in G.nodes()],\n",
        "    'betweenness_centrality': [betweenness_centrality[n] for n in G.nodes()]\n",
        "}).set_index('node_id')\n",
        "\n",
        "# Calculate Ranks (Required TANS Step 2 for Prompting)\n",
        "for col in node_properties_df.columns:\n",
        "    node_properties_df[f'{col}_rank'] = node_properties_df[col].rank(pct=True) * 100\n",
        "\n",
        "# --- Global Variables for Pubmed Prompting ---\n",
        "# Pubmed has 3 classes (e.g., specific types of diabetic papers)\n",
        "classes_pubmed = [\"Experimental Diabetes\", \"Diabetes Mellitus\", \"Type 1 Diabetes\"] # Placeholder for actual class names\n",
        "sample_node_id = list(G.nodes())[500]\n",
        "sample_text = \"A paper discussing a novel finding related to insulin resistance in mice.\"\n",
        "\n",
        "print(\"Pubmed setup complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gJcqKsE50zq",
        "outputId": "8866be12-3db7-49ae-93fd-ffdda4de1cf9"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Loading Pubmed Dataset ---\n",
            "Nodes: 19717, Original Features: 500, Classes: 3\n",
            "Pubmed setup complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "\n",
        "# --- 1. SIMULATE PUBMED TEXT CONTENT ---\n",
        "# The original texts (abstracts/titles) are not included in the PyG Planetoid object.\n",
        "# We must simulate a DataFrame that holds the text content, similar to your 'cora_content'.\n",
        "\n",
        "# Get all node IDs from your NetworkX graph\n",
        "all_node_ids = list(G.nodes())\n",
        "\n",
        "# Create placeholder texts for demonstration purposes\n",
        "# In a real scenario, you would load the actual Pubmed abstracts/titles here.\n",
        "def generate_placeholder_text(node_id):\n",
        "    \"\"\"Generates a unique placeholder abstract for a node.\"\"\"\n",
        "    # Use the node's true class from the data tensor for a more realistic simulation\n",
        "    node_index = list(G.nodes()).index(node_id)\n",
        "    true_label = data.y[node_index].item()\n",
        "\n",
        "    # Class labels from your setup: [\"Experimental Diabetes\", \"Diabetes Mellitus\", \"Type 1 Diabetes\"]\n",
        "    class_name = classes_pubmed[true_label]\n",
        "\n",
        "    # Generate the text\n",
        "    text = (\n",
        "        f\"Research paper ID {node_id}. This article, categorized as '{class_name}', \"\n",
        "        f\"investigates the efficacy of novel genetic biomarkers in predicting \"\n",
        "        f\"long-term outcomes for patients with {class_name}. \"\n",
        "        f\"The findings suggest a strong correlation between the expression of \"\n",
        "        f\"the GNG-23 receptor and improved therapeutic response.\"\n",
        "    )\n",
        "    return text\n",
        "\n",
        "# Create the simulated content DataFrame\n",
        "pubmed_texts_df = pd.DataFrame({\n",
        "    'id': all_node_ids,\n",
        "    'text': [generate_placeholder_text(n) for n in all_node_ids]\n",
        "}).set_index('id')\n",
        "\n",
        "print(\"Simulated Pubmed Text Content (pubmed_texts_df) created.\")\n",
        "\n",
        "# --- 2. DEFINE GET_ORIGINAL_TEXT ---\n",
        "def get_pubmed_text(node_id):\n",
        "    \"\"\"\n",
        "    Retrieves the simulated original text (abstract/title) for a given node ID.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return pubmed_texts_df.loc[node_id, 'text']\n",
        "    except KeyError:\n",
        "        return f\"Text content not found for node ID {node_id}.\"\n",
        "\n",
        "\n",
        "# --- 3. DEFINE GET_NEIGHBOR_TEXTS (ADAPTED) ---\n",
        "def get_neighbor_texts_pubmed(graph, node_id, num_neighbors=5):\n",
        "    \"\"\"\n",
        "    Retrieves up to num_neighbors simulated texts of connected nodes for Pubmed.\n",
        "    \"\"\"\n",
        "    if node_id not in graph:\n",
        "        return \"Node not found in graph.\"\n",
        "\n",
        "    neighbors = list(graph.neighbors(node_id))\n",
        "\n",
        "    if len(neighbors) == 0:\n",
        "        return \"No connected nodes found.\"\n",
        "\n",
        "    # Use NumPy random choice for selecting neighbors\n",
        "    selected = np.random.choice(\n",
        "        neighbors,\n",
        "        min(num_neighbors, len(neighbors)),\n",
        "        replace=False\n",
        "    )\n",
        "\n",
        "    # Format the output string\n",
        "    return \"\\n\".join(\n",
        "        [\n",
        "            f\"Node {n}: {get_pubmed_text(n)[:150]}...\"\n",
        "            for n in selected\n",
        "        ]\n",
        "    )\n",
        "\n",
        "# --- 4. TEST WITH A SAMPLE NODE ---\n",
        "# We'll use the sample_node_id you defined earlier\n",
        "test_node = sample_node_id\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\"--- TEST RESULTS FOR PUBMED (Node ID: {test_node}) ---\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(\"--- Original Text ---\")\n",
        "print(get_pubmed_text(test_node))\n",
        "\n",
        "print(\"\\n--- Neighbor Texts ---\")\n",
        "# Use the graph G you created and the new function\n",
        "print(get_neighbor_texts_pubmed(G, test_node, num_neighbors=5))"
      ],
      "metadata": {
        "id": "fD6pdeBfMnYY",
        "outputId": "29fbcf5c-6349-45c5-ccfc-1169a3abae7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simulated Pubmed Text Content (pubmed_texts_df) created.\n",
            "\n",
            "==================================================\n",
            "--- TEST RESULTS FOR PUBMED (Node ID: 500) ---\n",
            "==================================================\n",
            "--- Original Text ---\n",
            "Research paper ID 500. This article, categorized as 'Type 1 Diabetes', investigates the efficacy of novel genetic biomarkers in predicting long-term outcomes for patients with Type 1 Diabetes. The findings suggest a strong correlation between the expression of the GNG-23 receptor and improved therapeutic response.\n",
            "\n",
            "--- Neighbor Texts ---\n",
            "Node 3831: Research paper ID 3831. This article, categorized as 'Type 1 Diabetes', investigates the efficacy of novel genetic biomarkers in predicting long-term ...\n",
            "Node 17301: Research paper ID 17301. This article, categorized as 'Type 1 Diabetes', investigates the efficacy of novel genetic biomarkers in predicting long-term...\n",
            "Node 14008: Research paper ID 14008. This article, categorized as 'Type 1 Diabetes', investigates the efficacy of novel genetic biomarkers in predicting long-term...\n",
            "Node 18728: Research paper ID 18728. This article, categorized as 'Type 1 Diabetes', investigates the efficacy of novel genetic biomarkers in predicting long-term...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Use the node_properties_df created earlier, which contains centrality scores\n",
        "# and ranks for all Pubmed nodes.\n",
        "\n",
        "print(\"Finding Most Influential Node\")\n",
        "\n",
        "influential_node_info = node_properties_df.sort_values(\n",
        "    'betweenness_centrality',\n",
        "    ascending=False\n",
        ").iloc[0]\n",
        "\n",
        "# Get the ID of the most central node\n",
        "most_central_node = influential_node_info.name\n",
        "\n",
        "print(f\"The node with the highest Betweenness Centrality is: {most_central_node}\")\n",
        "print(f\"Betweenness Centrality Score: {influential_node_info['betweenness_centrality']:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\"TEST RESULTS FOR PUBMED (Node ID: {most_central_node} - Highly Central)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Print the text of the central article\n",
        "print(\"Central Article Text\")\n",
        "print(get_pubmed_text(most_central_node))\n",
        "\n",
        "# Print the text of the articles that cite this central article (its neighbors)\n",
        "print(\"\\n Neighbor Texts (Citing/Cited Articles) \")\n",
        "print(get_neighbor_texts_pubmed(G, most_central_node, num_neighbors=5))"
      ],
      "metadata": {
        "id": "E_tyOAVUOG4c",
        "outputId": "f68ceeef-3744-4a54-8f31-408db8b49e2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Finding Most Influential Node ---\n",
            "The node with the highest Betweenness Centrality is: 11024\n",
            "Betweenness Centrality Score: 0.1429\n",
            "\n",
            "==================================================\n",
            "TEST RESULTS FOR PUBMED (Node ID: 11024 - Highly Central)\n",
            "==================================================\n",
            "Central Article Text\n",
            "Research paper ID 11024. This article, categorized as 'Type 1 Diabetes', investigates the efficacy of novel genetic biomarkers in predicting long-term outcomes for patients with Type 1 Diabetes. The findings suggest a strong correlation between the expression of the GNG-23 receptor and improved therapeutic response.\n",
            "\n",
            " Neighbor Texts (Citing/Cited Articles) \n",
            "Node 574: Research paper ID 574. This article, categorized as 'Type 1 Diabetes', investigates the efficacy of novel genetic biomarkers in predicting long-term o...\n",
            "Node 1416: Research paper ID 1416. This article, categorized as 'Diabetes Mellitus', investigates the efficacy of novel genetic biomarkers in predicting long-ter...\n",
            "Node 9723: Research paper ID 9723. This article, categorized as 'Type 1 Diabetes', investigates the efficacy of novel genetic biomarkers in predicting long-term ...\n",
            "Node 2783: Research paper ID 2783. This article, categorized as 'Experimental Diabetes', investigates the efficacy of novel genetic biomarkers in predicting long...\n",
            "Node 18830: Research paper ID 18830. This article, categorized as 'Type 1 Diabetes', investigates the efficacy of novel genetic biomarkers in predicting long-term...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super().__init__()\n",
        "        # First GCN layer: takes node features (in_channels) and outputs hidden_channels\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        # Second GCN layer: takes hidden_channels and outputs the number of classes (out_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        # 1. First Convolutional Layer\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)  # Apply ReLU activation function\n",
        "        x = F.dropout(x, p=0.5, training=self.training) # Dropout for regularization\n",
        "\n",
        "        # 2. Second Convolutional Layer\n",
        "        x = self.conv2(x, edge_index)\n",
        "\n",
        "        # 3. Final Output (Log Softmax for classification)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "Acl5uHJCNiy4"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you ran the initial PyG loading step:\n",
        "# dataset = Planetoid(root='./data/Pubmed', name='Pubmed')\n",
        "# data = dataset[0]\n",
        "\n",
        "# Model Parameters\n",
        "INPUT_DIM = data.num_node_features # 500 features (Bag-of-Words vectors)\n",
        "HIDDEN_DIM = 16                    # Standard hidden layer size for GCNs\n",
        "OUTPUT_DIM = dataset.num_classes   # 3 classes (types of papers)\n",
        "\n",
        "# Initialize the GCN model\n",
        "model = GCN(in_channels=INPUT_DIM, hidden_channels=HIDDEN_DIM, out_channels=OUTPUT_DIM)\n",
        "\n",
        "# Define the optimizer and loss function\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = torch.nn.NLLLoss() # Negative Log Likelihood Loss (matches log_softmax output)\n",
        "\n",
        "print(f\"GCN Model initialized with: Input={INPUT_DIM}, Hidden={HIDDEN_DIM}, Output={OUTPUT_DIM}\")"
      ],
      "metadata": {
        "id": "7JEcZ6bgNkEP",
        "outputId": "298cf98c-a6d2-486b-ae03-c48c4b1321c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCN Model initialized with: Input=500, Hidden=16, Output=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    model.train() # Set the model to training mode\n",
        "    optimizer.zero_grad() # Clear gradients\n",
        "\n",
        "    # Forward pass: Pass node features (x) and graph structure (edge_index)\n",
        "    out = model(data.x, data.edge_index)\n",
        "\n",
        "    # Calculate loss only on the nodes designated for training\n",
        "    # data.train_mask is a boolean mask provided by the Planetoid dataset\n",
        "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()"
      ],
      "metadata": {
        "id": "u4jgTScXNmdG"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "    model.eval() # Set the model to evaluation mode\n",
        "    out = model(data.x, data.edge_index)\n",
        "\n",
        "    # Get the predicted class (index of the max log probability)\n",
        "    pred = out.argmax(dim=1)\n",
        "\n",
        "    accuracies = {}\n",
        "\n",
        "    # Evaluate on the validation mask\n",
        "    correct_val = pred[data.val_mask] == data.y[data.val_mask]\n",
        "    accuracies['val'] = int(correct_val.sum()) / int(data.val_mask.sum())\n",
        "\n",
        "    # Evaluate on the test mask\n",
        "    correct_test = pred[data.test_mask] == data.y[data.test_mask]\n",
        "    accuracies['test'] = int(correct_test.sum()) / int(data.test_mask.sum())\n",
        "\n",
        "    return accuracies"
      ],
      "metadata": {
        "id": "oVLZ0iUqNqIA"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Starting GNN Training ---\")\n",
        "for epoch in range(1, 201):\n",
        "    loss = train()\n",
        "    accs = test()\n",
        "\n",
        "    if epoch % 20 == 0 or epoch == 1:\n",
        "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
        "              f'Val Acc: {accs[\"val\"]:.4f}, Test Acc: {accs[\"test\"]:.4f}')\n",
        "\n",
        "print(\"--- Training Complete ---\")"
      ],
      "metadata": {
        "id": "xclXSUg2NtTU",
        "outputId": "df13c7e4-d1c8-4ff5-f7dd-c02b4dc0ff3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting GNN Training ---\n",
            "Epoch: 001, Loss: 1.0925, Val Acc: 0.5900, Test Acc: 0.6150\n",
            "Epoch: 020, Loss: 0.7055, Val Acc: 0.7360, Test Acc: 0.6970\n",
            "Epoch: 040, Loss: 0.3440, Val Acc: 0.7580, Test Acc: 0.7600\n",
            "Epoch: 060, Loss: 0.2268, Val Acc: 0.7840, Test Acc: 0.7740\n",
            "Epoch: 080, Loss: 0.1267, Val Acc: 0.7900, Test Acc: 0.7810\n",
            "Epoch: 100, Loss: 0.1213, Val Acc: 0.7820, Test Acc: 0.7800\n",
            "Epoch: 120, Loss: 0.1242, Val Acc: 0.7840, Test Acc: 0.7840\n",
            "Epoch: 140, Loss: 0.1042, Val Acc: 0.7880, Test Acc: 0.7850\n",
            "Epoch: 160, Loss: 0.1096, Val Acc: 0.7880, Test Acc: 0.7950\n",
            "Epoch: 180, Loss: 0.0856, Val Acc: 0.7820, Test Acc: 0.7900\n",
            "Epoch: 200, Loss: 0.0791, Val Acc: 0.7880, Test Acc: 0.7950\n",
            "--- Training Complete ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- USA Airport Network (Text-Free) Setup ---\n",
        "\n",
        "# This dataset is not part of Planetoid. You would need to load it from a source\n",
        "# like the official TANS repository or the original data source (e.g., using NetworkX\n",
        "# after downloading the edge list).\n",
        "# --- Conceptual Loading ---\n",
        "# G_usa = nx.read_edgelist('usa.edgelist', nodetype=int)\n",
        "\n",
        "# --- MOCKING DATA FOR CONTINUITY ---\n",
        "# Since direct loading is complex, we mock a small text-free graph for demonstration.\n",
        "G_usa_mock = nx.random_geometric_graph(n=1190, radius=0.1) # Mock USA graph (1,190 nodes)\n",
        "\n",
        "# Calculate Centralities\n",
        "degree_centrality_usa = nx.degree_centrality(G_usa_mock)\n",
        "\n",
        "classes_usa = [\"High Activity\", \"Moderate Activity\", \"Moderately Low Activity\", \"Low Activity\"] # 4 classes\n",
        "sample_node_id_usa = list(G_usa_mock.nodes())[50]\n",
        "sample_text_usa = \"\"\n"
      ],
      "metadata": {
        "id": "IwJ_ItpAT5ew"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_original_text_text_free(node_id):\n",
        "    \"\"\"Returns empty string for text-free graphs.\"\"\"\n",
        "    return \"\"\n",
        "\n",
        "def get_neighbor_texts_text_free(graph, node_id, num_neighbors=5):\n",
        "    \"\"\"Returns a placeholder indicating no neighbor text exists.\"\"\"\n",
        "    return \"No textual descriptions available for connected nodes.\"\n",
        "\n",
        "\n",
        "# --- Example Prompt for a Text-Free Graph (USA) ---\n",
        "\n",
        "# 1. Extract node properties as a *DataFrame*, not a Series\n",
        "high_activity_props = node_properties_df.loc[[sample_node_id_usa]].copy()\n",
        "\n",
        "# 2. Add new values safely\n",
        "high_activity_props.loc[sample_node_id_usa, \"degree_centrality\"] = 0.1749\n",
        "high_activity_props.loc[sample_node_id_usa, \"degree_centrality_rank\"] = 99.58  # mock rank\n",
        "\n",
        "# 3. Now generate the prompt using the retained 2D structure\n",
        "final_prompt_usa = generate_tans_prompt(\n",
        "    G_usa_mock,\n",
        "    sample_node_id_usa,\n",
        "    high_activity_props,\n",
        "    classes_usa\n",
        ")\n",
        "\n",
        "print(\"\\n--- Final Generated TANS Prompt Example (USA - Text-Free) ---\")\n",
        "print(final_prompt_usa)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-3gZFoGT79H",
        "outputId": "48b198d8-9a96-4f38-e8c8-61d408a6d2d9"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Final Generated TANS Prompt Example (USA - Text-Free) ---\n",
            "Given a node from a citation network graph, where the node type is paper.\n",
            "The original node description is: \"No text found for this node.\".\n",
            "\n",
            "The following are the textual information of 5 connected nodes. The descriptions are:\n",
            "744: No text found for this node....\n",
            "518: No text found for this node....\n",
            "952: No text found for this node....\n",
            "532: No text found for this node....\n",
            "226: No text found for this node....\n",
            "\n",
            "Node Properties:\n",
            "- Degree Centrality value: 0.1749, ranked as 99.58% among all nodes.\n",
            "- Closeness Centrality value: 0.1458.\n",
            "- Betweenness Centrality value: 0.0000.\n",
            "\n",
            "Output the potential class of the node among the following classes: ['High Activity', 'Moderate Activity', 'Moderately Low Activity', 'Low Activity']. \n",
            "Provide reasons for your assessment. Your answer should be less than 200 words.\n"
          ]
        }
      ]
    }
  ]
}