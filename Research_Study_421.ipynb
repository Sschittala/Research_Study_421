{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mVEArGxNd_sU",
    "outputId": "460eb47d-25a6-48f7-86cd-08453f34734c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://data.pyg.org/whl/torch-2.9.0+cu126.html\n",
      "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.12/dist-packages (2.1.2)\n",
      "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.12/dist-packages (2.7.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.13.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2025.3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.1.6)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.0.2)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (5.9.5)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.2.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.32.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.22.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch-geometric) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2025.11.12)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch-geometric) (4.15.0)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "TORCH = torch.__version__.split('+')[0]\n",
    "CUDA = 'cu' + torch.version.cuda.replace('.', '')\n",
    "\n",
    "# 2. Install torch-scatter, torch-sparse, and finally, torch-geometric\n",
    "!pip install torch-scatter -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
    "!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TsRa1G6rdSIM",
    "outputId": "bfaeebbd-4ff4-4435-9629-c8dc9b8400f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes: 2708\n",
      "Features: 1433\n",
      "Classes: 7\n",
      "\n",
      "Total number of directed edges: 10556\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the Cora citation network dataset using PyTorch Geometric's Planetoid loader.\n",
    "# The 'Cora' dataset consists of machine learning papers classified into seven categories.\n",
    "dataset = Planetoid(root='./data/Cora', name='Cora')\n",
    "# 'data' is a torch_geometric.data.Data object containing the graph structure and features.\n",
    "data = dataset[0]\n",
    "\n",
    "# Convert the PyG Data object to a NetworkX graph (G) for using built-in NetworkX\n",
    "# graph analysis functions, specifically making it undirected for centrality calculations.\n",
    "G = to_networkx(data, to_undirected=True)\n",
    "\n",
    "# Print basic statistics about the Cora dataset\n",
    "print(f\"Nodes: {data.num_nodes}\") # Total number of nodes (papers)\n",
    "print(f\"Features: {data.num_node_features}\") # Dimension of the feature vector (Bag-of-Words) for each node\n",
    "print(f\"Classes: {dataset.num_classes}\") # Number of distinct classes (categories)\n",
    "\n",
    "# 'edge_index' stores the graph connectivity in COO format (2 x num_edges tensor).\n",
    "edge_index = data.edge_index\n",
    "\n",
    "\n",
    "# Set a limit for printing edges (though no edges are printed in the final output)\n",
    "num_edges_to_print = min(5, edge_index.shape[1])\n",
    "\n",
    "# Print the total number of edges (directed pairs)\n",
    "print(f\"\\nTotal number of directed edges: {data.num_edges}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q7EmqHrapGyy",
    "outputId": "174a593f-040f-4ce2-a575-86c9e6a26ae2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Node Properties:\n",
      "         degree_centrality  closeness_centrality  betweenness_centrality\n",
      "node_id                                                                 \n",
      "0                 0.001108              0.144255            9.766154e-07\n",
      "1                 0.001108              0.151453            1.080477e-03\n",
      "2                 0.001847              0.179168            4.050816e-03\n",
      "3                 0.000369              0.000369            0.000000e+00\n",
      "4                 0.001847              0.153266            5.511762e-04\n"
     ]
    }
   ],
   "source": [
    "# Calculate Degree Centrality: Measures the number of connections (neighbors) a node has.\n",
    "# For an undirected graph, this indicates how many other papers a given paper cites or is cited by.\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "\n",
    "# Calculate Closeness Centrality: Measures how quickly a node can reach all other nodes in the network.\n",
    "# A higher score means the node is more central in terms of reachability and information flow speed.\n",
    "closeness_centrality = nx.closeness_centrality(G)\n",
    "\n",
    "# Calculate Betweenness Centrality: Measures how often a node lies on the shortest path between two other nodes.\n",
    "# A high score indicates the node acts as an important 'bridge' or intermediary in the network.\n",
    "betweenness_centrality = nx.betweenness_centrality(G)\n",
    "\n",
    "# Consolidate the calculated centrality scores into a single Pandas DataFrame.\n",
    "node_properties_df = pd.DataFrame({\n",
    "    # Get all node IDs from the NetworkX graph\n",
    "    'node_id': list(G.nodes()),\n",
    "    # Map the centrality dictionary results back to the node IDs\n",
    "    'degree_centrality': [degree_centrality[n] for n in G.nodes()],\n",
    "    'closeness_centrality': [closeness_centrality[n] for n in G.nodes()],\n",
    "    'betweenness_centrality': [betweenness_centrality[n] for n in G.nodes()]\n",
    "}).set_index('node_id') # Set the 'node_id' column as the DataFrame index\n",
    "\n",
    "# Print the first few rows of the DataFrame to inspect the calculated properties.\n",
    "print(\"Sample Node Properties:\")\n",
    "print(node_properties_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UiDHQYGxzKwI",
    "outputId": "eb44d556-698b-4936-dfa5-856c9c3cc3f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Node Properties with Ranks:\n",
      "         degree_centrality  degree_centrality_rank\n",
      "node_id                                           \n",
      "0                 0.001108               49.667651\n",
      "1                 0.001108               49.667651\n",
      "2                 0.001847               79.431315\n",
      "3                 0.000369                8.973412\n",
      "4                 0.001847               79.431315\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for col in node_properties_df.columns:\n",
    "    node_properties_df[f'{col}_rank'] = node_properties_df[col].rank(pct=True) * 100\n",
    "\n",
    "print(\"\\nSample Node Properties with Ranks:\")\n",
    "print(node_properties_df[['degree_centrality', 'degree_centrality_rank']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Function to align (reduce the dimensionality of) feature vectors using SVD.\n",
    "# feature_matrix: The input feature matrix (Nodes x Features).\n",
    "# target_dim: The desired number of dimensions for the output features.\n",
    "def align_features_with_svd(feature_matrix: np.ndarray, target_dim: int) -> np.ndarray:\n",
    "    # Check if the input feature matrix is empty.\n",
    "    if feature_matrix.size == 0:\n",
    "        print(\"Error: Feature matrix is empty. SVD requires node features.\")\n",
    "        return np.array([])\n",
    "        \n",
    "    # Step 1: Normalize the feature vectors (rows) using L2 norm. \n",
    "    # This ensures all input vectors have unit length before SVD.\n",
    "    normalized_features = normalize(feature_matrix, axis=1, norm='l2')\n",
    "\n",
    "    # Step 2: Perform Singular Value Decomposition (SVD).\n",
    "    # Decomposes normalized_features into U, S, and V_transpose.\n",
    "    # U: Left singular vectors (orthogonal basis for the column space).\n",
    "    # S: Singular values (magnitudes of the basis vectors).\n",
    "    # Vt: Right singular vectors (transposed, orthogonal basis for the row space).\n",
    "    U, S, Vt = np.linalg.svd(normalized_features, full_matrices=False)\n",
    "    \n",
    "    # Step 3: Determine the rank (r) for truncation.\n",
    "    # r is the minimum of the desired target dimension and the number of columns in U.\n",
    "    r = min(target_dim, U.shape[1])\n",
    "    \n",
    "    # Truncate U: Select the first 'r' columns corresponding to the largest singular values.\n",
    "    U_r = U[:, :r]\n",
    "    \n",
    "    # Truncate S: Select the first 'r' singular values and form a diagonal matrix.\n",
    "    S_r = np.diag(S[:r])\n",
    "    \n",
    "    # Step 4: Compute the aligned features using the product of U_r and S_r.\n",
    "    # This represents the dimensionality-reduced feature set.\n",
    "    aligned_features = U_r @ S_r\n",
    "\n",
    "    # Print the shapes for verification and debugging.\n",
    "    print(f\"Original shape: {feature_matrix.shape}\")\n",
    "    print(f\"Aligned shape: {aligned_features.shape}\")\n",
    "\n",
    "    return aligned_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Cora_features = np.random.rand(2708, 1433).astype(np.float32)\n",
    "\n",
    "# Pubmed: 19717 nodes, 500 features\n",
    "Pubmed_features = np.random.rand(19717, 500).astype(np.float32)\n",
    "\n",
    "TARGET_ALIGN_DIM = 128\n",
    "\n",
    "print(\"--- Aligning Cora Features ---\")\n",
    "aligned_cora = align_features_with_svd(Cora_features, TARGET_ALIGN_DIM)\n",
    "\n",
    "print(\"\\n--- Aligning Pubmed Features ---\")\n",
    "aligned_pubmed = align_features_with_svd(Pubmed_features, TARGET_ALIGN_DIM)\n",
    "\n",
    "# Output of above print statements\n",
    "##--- Aligning Cora Features ---\n",
    "## Original shape: (2708, 1433)\n",
    "## Aligned shape: (2708, 128)\n",
    "\n",
    "##--- Aligning Pubmed Features ---\n",
    "## Original shape: (19717, 500)\n",
    "## Aligned shape: (19717, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Jvf6HURzNaa"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# 1. Load Node Features and Labels (cora.content)\n",
    "# cora.content contains the node ID, a binary Bag-of-Words (BoW) feature vector, and the class label.\n",
    "# It is read as a TSV (Tab Separated Values) file without a header.\n",
    "cora_content = pd.read_csv(\"/content/cora/cora.content\", sep=\"\\t\", header=None)\n",
    "\n",
    "# Assign custom column names to make the data understandable:\n",
    "# - \"id\": The paper's unique identifier (node ID).\n",
    "# - \"w{i}\": The 1433 binary feature words.\n",
    "# - \"class\": The final classification label (e.g., Neural Networks, Genetic Algorithms).\n",
    "cora_content.columns = [\"id\"] + [f\"w{i}\" for i in range(1, cora_content.shape[1]-1)] + [\"class\"]\n",
    "\n",
    "# 2. Load Edge Data (cora.cites)\n",
    "# cora.cites defines the citation relationships (edges).\n",
    "edges = pd.read_csv(\"/content/cora/cora.cites\", sep=\"\\t\", header=None, names=[\"source\", \"target\"])\n",
    "\n",
    "# 3. Create the NetworkX Graph\n",
    "# Build a NetworkX graph G from the list of citation edges.\n",
    "# The graph is explicitly created as a Directed Graph (DiGraph) since citations are directional.\n",
    "G = nx.from_pandas_edgelist(edges, source=\"source\", target=\"target\", create_using=nx.DiGraph())\n",
    "\n",
    "# 4. Define Text Reconstruction Function\n",
    "# The original features are a binary vector (1=word present, 0=word absent).\n",
    "# This function reconstructs a pseudo-text string by listing all words (column names starting with 'w') that have a value of 1.\n",
    "def bow_to_text(row):\n",
    "    words = [col for col in row.index if col.startswith(\"w\") and row[col] == 1]\n",
    "    return \" \".join(words)\n",
    "\n",
    "# 5. Map Node IDs to Reconstructed Text\n",
    "# Create a dictionary mapping each paper's unique ID to its reconstructed pseudo-text string.\n",
    "node_text = {\n",
    "    row[\"id\"]: bow_to_text(row)  \n",
    "    for _, row in cora_content.iterrows()\n",
    "}\n",
    "\n",
    "# 6. Utility Function for Text Retrieval\n",
    "# Provides safe access to the reconstructed text for any given node ID.\n",
    "def get_original_text(node_id):\n",
    "    if node_id not in node_text:\n",
    "      return \"No text found for this node.\"\n",
    "    return node_text[node_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "DDbo_tsuZKiK"
   },
   "outputs": [],
   "source": [
    "common_nodes = list(set(G.nodes()).intersection(node_text.keys()))\n",
    "sample_node_id = common_nodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xl_60SlSVvsf",
    "outputId": "1cf93258-65af-47ef-e128-eb86d498c196"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Original Text ---\n",
      "w119 w126 w177 w253 w352 w457 w508 w522 w620 w649 w699 w703 w735 w846 w903 w1206 w1210 w1237 w1353 w1427\n",
      "\n",
      "--- Neighbor Texts ---\n",
      "686532: w133 w174 w212 w329 w330 w336 w435 w522 w565 w704 w726 w730 w798 w1171 w1209 w1212 w1258 w1302 w1329 w1340 w1424 w1426...\n",
      "31349: w457 w649 w903 w1210 w1274...\n",
      "1129442: w133 w136 w232 w238 w251 w265 w331 w469 w699 w875 w903 w1020 w1098 w1136 w1274 w1349 w1353 w1360...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to retrieve the text content of a sample of connected neighbor nodes\n",
    "# It's crucial for generating the Text-Attributed Node Sample (TANS) prompt, \n",
    "# as neighbor text provides contextual information for classification.\n",
    "def get_neighbor_texts(graph, node_id, num_neighbors=5):\n",
    "    \n",
    "    # 1. Check if the target node exists in the graph.\n",
    "    if node_id not in graph:\n",
    "        return \"Node not found in graph.\"\n",
    "\n",
    "    # 2. Get the list of all connected nodes (neighbors).\n",
    "    neighbors = list(graph.neighbors(node_id))\n",
    "\n",
    "    # 3. Handle case where the node has no connections.\n",
    "    if len(neighbors) == 0:\n",
    "        return \"No connected nodes found.\"\n",
    "\n",
    "    # 4. Randomly select a fixed number of neighbors (up to num_neighbors).\n",
    "    selected = np.random.choice(\n",
    "        neighbors,\n",
    "        min(num_neighbors, len(neighbors)), # Ensure we don't try to sample more neighbors than exist\n",
    "        replace=False # Do not pick the same neighbor twice\n",
    "    )\n",
    "\n",
    "    # 5. Format and return the neighbor texts.\n",
    "    # It constructs a string where each line contains the neighbor ID and a truncated version of its text.\n",
    "    return \"\\n\".join(\n",
    "        [\n",
    "            # 'get_original_text(n)' is assumed to be a previously defined function\n",
    "            f\"{n}: {get_original_text(n)[:200]}...\" \n",
    "            for n in selected\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# --- Demonstration and Testing ---\n",
    "\n",
    "# Select a test node (using the ID of the first paper in the loaded cora.content file)\n",
    "# 'cora_content' and 'get_original_text' are assumed to be defined in previous cells.\n",
    "test_node = cora_content.iloc[0][\"id\"]\n",
    "\n",
    "# Print the text content of the target node\n",
    "print(\"--- Original Text ---\")\n",
    "print(get_original_text(test_node))\n",
    "\n",
    "# Print the text content of the sample node's neighbors\n",
    "print(\"\\n--- Neighbor Texts ---\")\n",
    "# 'G' is the NetworkX graph loaded previously\n",
    "print(get_neighbor_texts(G, test_node, num_neighbors=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-yS0SiSrzU9Z"
   },
   "outputs": [],
   "source": [
    "# Function to assemble the Text-Attributed Node Sample (TANS) prompt for the LLM.\n",
    "# It packages textual and structural information into a structured request.\n",
    "def generate_tans_prompt(graph, node_id, properties_df, classes):\n",
    "    \n",
    "    # Retrieve pre-calculated centrality and rank values for the target node.\n",
    "    degree = properties_df.loc[node_id, 'degree_centrality']\n",
    "    rank_degree = properties_df.loc[node_id, 'degree_centrality_rank']\n",
    "\n",
    "    # Get the text for the target node (assumes 'get_original_text' is defined).\n",
    "    original_text = get_original_text(node_id)\n",
    "\n",
    "    # Get the text for a sample of the neighbor nodes (assumes 'get_neighbor_texts' is defined).\n",
    "    neighbor_texts = get_neighbor_texts(graph, node_id, num_neighbors=5)\n",
    "\n",
    "    # Construct the final prompt using an f-string template.\n",
    "    prompt = f\"\"\"\n",
    "Given a node from a citation network graph, where the node type is paper.\n",
    "The original node description is: \"{original_text}\".\n",
    "\n",
    "The following are the textual information of 5 connected nodes. The descriptions are:\n",
    "{neighbor_texts}\n",
    "\n",
    "Node Properties:\n",
    "# Include key structural metrics and their ranks.\n",
    "- Degree Centrality value: {degree:.4f}, ranked as {rank_degree:.2f}% among all nodes.\n",
    "- Closeness Centrality value: {properties_df.loc[node_id, 'closeness_centrality']:.4f}.\n",
    "- Betweenness Centrality value: {properties_df.loc[node_id, 'betweenness_centrality']:.4f}.\n",
    "\n",
    "# Final instruction: specify the task, the class list, the required reasoning, and the length constraint.\n",
    "Output the potential class of the node among the following classes: {classes}.\n",
    "Provide reasons for your assessment. Your answer should be less than 200 words.\n",
    "\"\"\"\n",
    "    # Remove leading/trailing whitespace and return the final prompt string.\n",
    "    return prompt.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XeaVuw3La1iC"
   },
   "outputs": [],
   "source": [
    "\n",
    "common_nodes = list(\n",
    "    set(G.nodes()).intersection(node_text.keys()).intersection(node_properties_df.index)\n",
    ")\n",
    "\n",
    "# Pick a sample node\n",
    "sample_node_id = common_nodes[0]  # safe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O7f0Rcr_arkM",
    "outputId": "866f4c58-3e8e-463e-a416-e43aa28a7807"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample node ID: 128\n",
      "In G: True\n",
      "In node_text: True\n",
      "In node_properties_df: True\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample node ID:\", sample_node_id)\n",
    "print(\"In G:\", sample_node_id in G)\n",
    "print(\"In node_text:\", sample_node_id in node_text)\n",
    "print(\"In node_properties_df:\", sample_node_id in node_properties_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8jCHPh1vzWTr",
    "outputId": "4a7cec05-0500-49f3-db94-0b4386e06f7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Generated TANS Prompt Example ---\n",
      "Given a node from a citation network graph, where the node type is paper.\n",
      "The original node description is: \"w2 w42 w188 w213 w358 w405 w465 w506 w508 w582 w636 w875 w989 w1072 w1231 w1232 w1259 w1264 w1275 w1394\".\n",
      "\n",
      "The following are the textual information of 5 connected nodes. The descriptions are:\n",
      "20526: w100 w241 w331 w335 w549 w582 w633 w649 w830 w875 w1072 w1119 w1132 w1156 w1178 w1193 w1207 w1264 w1275 w1360 w1433...\n",
      "91975: w158 w212 w238 w357 w447 w521 w595 w605 w624 w649 w656 w724 w830 w875 w940 w1072 w1264 w1275 w1309 w1360 w1424...\n",
      "1114125: w94 w100 w335 w402 w582 w605 w774 w981 w1156 w1178 w1264 w1293 w1307 w1315 w1321 w1382...\n",
      "39403: w127 w293 w335 w549 w582 w605 w626 w774 w912 w973 w989 w1133 w1156 w1263 w1264 w1293 w1307 w1315 w1321 w1382...\n",
      "\n",
      "Node Properties:\n",
      "- Degree Centrality value: 0.0015, ranked as 67.06% among all nodes.\n",
      "- Closeness Centrality value: 0.1304.\n",
      "- Betweenness Centrality value: 0.0007.\n",
      "\n",
      "Output the potential class of the node among the following classes: ['Neural Networks', 'Probabilistic Methods', 'Genetic Algorithms', 'Theory', 'Case Based', 'Reinforcement Learning', 'Rule Learning']. \n",
      "Provide reasons for your assessment. Your answer should be less than 200 words.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "classes_cora = [\"Neural Networks\", \"Probabilistic Methods\", \"Genetic Algorithms\", \"Theory\", \"Case Based\", \"Reinforcement Learning\", \"Rule Learning\"]\n",
    "\n",
    "\n",
    "common_nodes = list(\n",
    "    set(G.nodes()).intersection(node_text.keys()).intersection(node_properties_df.index)\n",
    ")\n",
    "\n",
    "sample_node_id = common_nodes[0]\n",
    "\n",
    "final_prompt = generate_tans_prompt(\n",
    "    G,\n",
    "    sample_node_id,\n",
    "    node_properties_df,\n",
    "    classes_cora\n",
    ")\n",
    "print(\"\\n--- Final Generated TANS Prompt Example ---\")\n",
    "print(final_prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oXwJIj2qu22e",
    "outputId": "99b0cc61-e4f2-43db-ce80-dec02cd3b8d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: Neural Networks\n",
      "Gemini-Generated TANS Description:\n",
      "The node and its connected papers exhibit a strong overlap in their textual descriptions, with several anonymous keywords appearing frequently across multiple nodes. Most notably, `w1264` is present in all five papers, and `w582` appears in four. Keywords `w875`, `w1072`, and `w1275` are also shared extensively.\n",
      "\n",
      "This high co-occurrence of specific, technical terms suggests a focused research domain with a distinct vocabulary. The node's moderate Degree Centrality (67.06%) indicates it's well-connected within its community, while its low Betweenness Centrality suggests it's not bridging disparate areas. This profile aligns with a paper deeply embedded in a specialized field.\n",
      "\n",
      "Among the given options, **Neural Networks** are characterized by highly specific architectures, algorithms, and components, which often leads to a concentrated and consistently shared technical vocabulary among related papers. This pattern of focused, shared keywords strongly supports a classification in Neural Networks.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google import genai\n",
    "from google.genai.errors import APIError\n",
    "\n",
    "# Set the Gemini API key as an environment variable. \n",
    "# NOTE: The actual key must be replaced for the code to run.\n",
    "os.environ['GEMINI_API_KEY'] = 'YOUR_API_KEY_HERE'\n",
    "client = genai.Client()\n",
    "\n",
    "# Function to query the Gemini model with the TANS prompt and parse the result.\n",
    "def query_llm_and_generate_description_gemini(prompt, class_list):\n",
    "    try:\n",
    "        # Re-instantiate the client (redundant if using global 'client', but often done in functions)\n",
    "        client = genai.Client()\n",
    "\n",
    "        # Call the Gemini API to generate content based on the prompt.\n",
    "        response = client.models.generate_content(\n",
    "            model='gemini-2.5-flash', # Use the specified model\n",
    "            contents=prompt           # The TANS prompt containing text and structural data\n",
    "        )\n",
    "\n",
    "        llm_explanation = response.text\n",
    "        predicted_class = None\n",
    "        \n",
    "        # Simple heuristic to extract the predicted class: \n",
    "        # Check if any class name from the provided list appears in the LLM's explanation (case-insensitive).\n",
    "        for cls in class_list:\n",
    "            if cls.lower() in llm_explanation.lower():\n",
    "                predicted_class = cls\n",
    "                break\n",
    "\n",
    "        return predicted_class, llm_explanation\n",
    "\n",
    "    # Error handling for API-related issues\n",
    "    except APIError as e:\n",
    "        print(f\"Gemini API Error: {e}\")\n",
    "        return \"Error: Could not generate description due to API error.\", \"\"\n",
    "    # Generic error handling\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return \"Error: An unexpected error occurred.\", \"\"\n",
    "\n",
    "# Define the set of possible classes for the Cora dataset.\n",
    "classes_cora = [\n",
    "    \"Neural Networks\", \"Probabilistic Methods\", \"Genetic Algorithms\",\n",
    "    \"Theory\", \"Case Based\", \"Reinforcement Learning\", \"Rule Learning\"\n",
    "]\n",
    "\n",
    "# Run the LLM query using the previously generated 'final_prompt' (assumed to be available).\n",
    "# This executes the node classification using the LLM.\n",
    "predicted_class, llm_generated_text = query_llm_and_generate_description_gemini(final_prompt, classes_cora)\n",
    "\n",
    "# Output the final results: the extracted predicted class and the full LLM-generated reasoning.\n",
    "print(f\"Predicted Class: {predicted_class}\")\n",
    "print(f\"Gemini-Generated TANS Description:\\n{llm_generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-gJcqKsE50zq",
    "outputId": "8866be12-3db7-49ae-93fd-ffdda4de1cf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Pubmed Dataset ---\n",
      "Nodes: 19717, Original Features: 500, Classes: 3\n",
      "Pubmed setup complete.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the Pubmed dataset\n",
    "print(\"--- Loading Pubmed Dataset ---\")\n",
    "dataset = Planetoid(root='./data/Pubmed', name='Pubmed')\n",
    "data = dataset[0]\n",
    "print(f\"Nodes: {data.num_nodes}, Original Features: {data.num_node_features}, Classes: {dataset.num_classes}\")\n",
    "\n",
    "# Convert to NetworkX\n",
    "G = to_networkx(data, to_undirected=True)\n",
    "\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "closeness_centrality = nx.closeness_centrality(G)\n",
    "betweenness_centrality = nx.betweenness_centrality(G)\n",
    "\n",
    "node_properties_df = pd.DataFrame({\n",
    "    'node_id': list(G.nodes()),\n",
    "    'degree_centrality': [degree_centrality[n] for n in G.nodes()],\n",
    "    'closeness_centrality': [closeness_centrality[n] for n in G.nodes()],\n",
    "    'betweenness_centrality': [betweenness_centrality[n] for n in G.nodes()]\n",
    "}).set_index('node_id')\n",
    "\n",
    "for col in node_properties_df.columns:\n",
    "    node_properties_df[f'{col}_rank'] = node_properties_df[col].rank(pct=True) * 100\n",
    "\n",
    "\n",
    "classes_pubmed = [\"Experimental Diabetes\", \"Diabetes Mellitus\", \"Type 1 Diabetes\"] \n",
    "sample_node_id = list(G.nodes())[500]\n",
    "sample_text = \"A paper discussing a novel finding related to insulin resistance in mice.\"\n",
    "\n",
    "print(\"Pubmed setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fD6pdeBfMnYY",
    "outputId": "29fbcf5c-6349-45c5-ccfc-1169a3abae7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated Pubmed Text Content (pubmed_texts_df) created.\n",
      "\n",
      "==================================================\n",
      "--- TEST RESULTS FOR PUBMED (Node ID: 500) ---\n",
      "==================================================\n",
      "--- Original Text ---\n",
      "Research paper ID 500. This article, categorized as 'Type 1 Diabetes', investigates the efficacy of novel genetic biomarkers in predicting long-term outcomes for patients with Type 1 Diabetes. The findings suggest a strong correlation between the expression of the GNG-23 receptor and improved therapeutic response.\n",
      "\n",
      "--- Neighbor Texts ---\n",
      "Node 3831: Research paper ID 3831. This article, categorized as 'Type 1 Diabetes', investigates the efficacy of novel genetic biomarkers in predicting long-term ...\n",
      "Node 17301: Research paper ID 17301. This article, categorized as 'Type 1 Diabetes', investigates the efficacy of novel genetic biomarkers in predicting long-term...\n",
      "Node 14008: Research paper ID 14008. This article, categorized as 'Type 1 Diabetes', investigates the efficacy of novel genetic biomarkers in predicting long-term...\n",
      "Node 18728: Research paper ID 18728. This article, categorized as 'Type 1 Diabetes', investigates the efficacy of novel genetic biomarkers in predicting long-term...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# 'G' and 'data' (Pubmed PyG data object) are assumed to be loaded from previous cells.\n",
    "# 'all_node_ids' is a list of all node IDs in the Pubmed graph.\n",
    "all_node_ids = list(G.nodes())\n",
    "\n",
    "\n",
    "# Function to generate simulated, category-specific text content for a Pubmed node.\n",
    "# It uses the node's true class label (data.y) to embed a category name into the text.\n",
    "def generate_placeholder_text(node_id):\n",
    "    # Get the integer index of the node within the PyG data structure.\n",
    "    node_index = list(G.nodes()).index(node_id)\n",
    "    # Retrieve the true numerical class label (0, 1, or 2).\n",
    "    true_label = data.y[node_index].item()\n",
    "\n",
    "    # Map the numerical label to the class name string (assumes 'classes_pubmed' is defined).\n",
    "    class_name = classes_pubmed[true_label]\n",
    "\n",
    "    # Construct the simulated text string, mentioning the category for demonstration.\n",
    "    text = (\n",
    "        f\"Research paper ID {node_id}. This article, categorized as '{class_name}', \"\n",
    "        f\"investigates the efficacy of novel genetic biomarkers in predicting \"\n",
    "        f\"long-term outcomes for patients with {class_name}. \"\n",
    "        f\"The findings suggest a strong correlation between the expression of \"\n",
    "        f\"the GNG-23 receptor and improved therapeutic response.\"\n",
    "    )\n",
    "    return text\n",
    "\n",
    "# Create a DataFrame mapping every node ID to its generated placeholder text.\n",
    "pubmed_texts_df = pd.DataFrame({\n",
    "    'id': all_node_ids,\n",
    "    'text': [generate_placeholder_text(n) for n in all_node_ids]\n",
    "}).set_index('id') # Set the node ID as the index for fast lookup.\n",
    "\n",
    "print(\"Simulated Pubmed Text Content (pubmed_texts_df) created.\")\n",
    "\n",
    "# Utility function to retrieve the simulated text content for a specific node ID.\n",
    "def get_pubmed_text(node_id):\n",
    "    try:\n",
    "        return pubmed_texts_df.loc[node_id, 'text']\n",
    "    except KeyError:\n",
    "        return f\"Text content not found for node ID {node_id}.\"\n",
    "\n",
    "\n",
    "# Function to retrieve the simulated text content for a sample of the node's neighbors.\n",
    "def get_neighbor_texts_pubmed(graph, node_id, num_neighbors=5):\n",
    "    \n",
    "    # Check for node existence.\n",
    "    if node_id not in graph:\n",
    "        return \"Node not found in graph.\"\n",
    "\n",
    "    # Get neighbors.\n",
    "    neighbors = list(graph.neighbors(node_id))\n",
    "\n",
    "    # Handle case with no neighbors.\n",
    "    if len(neighbors) == 0:\n",
    "        return \"No connected nodes found.\"\n",
    "\n",
    "    # Randomly select a subset of neighbors.\n",
    "    selected = np.random.choice(\n",
    "        neighbors,\n",
    "        min(num_neighbors, len(neighbors)),\n",
    "        replace=False\n",
    "    )\n",
    "\n",
    "    # Format the neighbor output, retrieving and truncating the simulated text.\n",
    "    return \"\\n\".join(\n",
    "        [\n",
    "            f\"Node {n}: {get_pubmed_text(n)[:150]}...\" # Truncate text for prompt brevity\n",
    "            for n in selected\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# --- Demonstration and Testing ---\n",
    "\n",
    "# 'sample_node_id' is assumed to be defined in a previous cell (e.g., node ID 500).\n",
    "test_node = sample_node_id\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"--- TEST RESULTS FOR PUBMED (Node ID: {test_node}) ---\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Print the simulated text for the target node.\n",
    "print(\"--- Original Text ---\")\n",
    "print(get_pubmed_text(test_node))\n",
    "\n",
    "# Print the simulated text for a sample of the target node's neighbors.\n",
    "print(\"\\n--- Neighbor Texts ---\")\n",
    "print(get_neighbor_texts_pubmed(G, test_node, num_neighbors=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E_tyOAVUOG4c",
    "outputId": "f68ceeef-3744-4a54-8f31-408db8b49e2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Finding Most Influential Node ---\n",
      "The node with the highest Betweenness Centrality is: 11024\n",
      "Betweenness Centrality Score: 0.1429\n",
      "\n",
      "==================================================\n",
      "TEST RESULTS FOR PUBMED (Node ID: 11024 - Highly Central)\n",
      "==================================================\n",
      "Central Article Text\n",
      "Research paper ID 11024. This article, categorized as 'Type 1 Diabetes', investigates the efficacy of novel genetic biomarkers in predicting long-term outcomes for patients with Type 1 Diabetes. The findings suggest a strong correlation between the expression of the GNG-23 receptor and improved therapeutic response.\n",
      "\n",
      " Neighbor Texts (Citing/Cited Articles) \n",
      "Node 574: Research paper ID 574. This article, categorized as 'Type 1 Diabetes', investigates the efficacy of novel genetic biomarkers in predicting long-term o...\n",
      "Node 1416: Research paper ID 1416. This article, categorized as 'Diabetes Mellitus', investigates the efficacy of novel genetic biomarkers in predicting long-ter...\n",
      "Node 9723: Research paper ID 9723. This article, categorized as 'Type 1 Diabetes', investigates the efficacy of novel genetic biomarkers in predicting long-term ...\n",
      "Node 2783: Research paper ID 2783. This article, categorized as 'Experimental Diabetes', investigates the efficacy of novel genetic biomarkers in predicting long...\n",
      "Node 18830: Research paper ID 18830. This article, categorized as 'Type 1 Diabetes', investigates the efficacy of novel genetic biomarkers in predicting long-term...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "print(\"Finding Most Influential Node\")\n",
    "\n",
    "influential_node_info = node_properties_df.sort_values(\n",
    "    'betweenness_centrality',\n",
    "    ascending=False\n",
    ").iloc[0]\n",
    "\n",
    "most_central_node = influential_node_info.name\n",
    "\n",
    "print(f\"The node with the highest Betweenness Centrality is: {most_central_node}\")\n",
    "print(f\"Betweenness Centrality Score: {influential_node_info['betweenness_centrality']:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"TEST RESULTS FOR PUBMED (Node ID: {most_central_node} - Highly Central)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"Central Article Text\")\n",
    "print(get_pubmed_text(most_central_node))\n",
    "\n",
    "print(\"\\n Neighbor Texts (Citing/Cited Articles) \")\n",
    "print(get_neighbor_texts_pubmed(G, most_central_node, num_neighbors=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Acl5uHJCNiy4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)  \n",
    "        x = F.dropout(x, p=0.5, training=self.training) \n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7JEcZ6bgNkEP",
    "outputId": "b7f9e6bd-c204-4c20-f87e-ab44c7dd9dfa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN Model initialized with: Input=500, Hidden=16, Output=3\n"
     ]
    }
   ],
   "source": [
    "dataset = Planetoid(root='./data/Pubmed', name='Pubmed')\n",
    "data = dataset[0]\n",
    "\n",
    "INPUT_DIM = data.num_node_features \n",
    "HIDDEN_DIM = 16                    \n",
    "OUTPUT_DIM = dataset.num_classes   \n",
    "\n",
    "model = GCN(in_channels=INPUT_DIM, hidden_channels=HIDDEN_DIM, out_channels=OUTPUT_DIM)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.NLLLoss() \n",
    "\n",
    "print(f\"GCN Model initialized with: Input={INPUT_DIM}, Hidden={HIDDEN_DIM}, Output={OUTPUT_DIM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u4jgTScXNmdG"
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    out = model(data.x, data.edge_index)\n",
    "\n",
    "    \n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oVLZ0iUqNqIA"
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval() \n",
    "    out = model(data.x, data.edge_index)\n",
    "\n",
    "    pred = out.argmax(dim=1)\n",
    "\n",
    "    accuracies = {}\n",
    "\n",
    "    correct_val = pred[data.val_mask] == data.y[data.val_mask]\n",
    "    accuracies['val'] = int(correct_val.sum()) / int(data.val_mask.sum())\n",
    "\n",
    "    correct_test = pred[data.test_mask] == data.y[data.test_mask]\n",
    "    accuracies['test'] = int(correct_test.sum()) / int(data.test_mask.sum())\n",
    "\n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xclXSUg2NtTU",
    "outputId": "37d793e6-a3b6-42f2-b4b7-a0ed45f45dce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting GNN Training ---\n",
      "Epoch: 001, Loss: 1.1000, Val Acc: 0.4880, Test Acc: 0.4530\n",
      "Epoch: 020, Loss: 0.7029, Val Acc: 0.7480, Test Acc: 0.7380\n",
      "Epoch: 040, Loss: 0.3602, Val Acc: 0.7680, Test Acc: 0.7580\n",
      "Epoch: 060, Loss: 0.2166, Val Acc: 0.7820, Test Acc: 0.7730\n",
      "Epoch: 080, Loss: 0.1287, Val Acc: 0.7760, Test Acc: 0.7770\n",
      "Epoch: 100, Loss: 0.1235, Val Acc: 0.7780, Test Acc: 0.7780\n",
      "Epoch: 120, Loss: 0.0989, Val Acc: 0.7760, Test Acc: 0.7740\n",
      "Epoch: 140, Loss: 0.1132, Val Acc: 0.7760, Test Acc: 0.7790\n",
      "Epoch: 160, Loss: 0.1023, Val Acc: 0.7840, Test Acc: 0.7820\n",
      "Epoch: 180, Loss: 0.0943, Val Acc: 0.7820, Test Acc: 0.7810\n",
      "Epoch: 200, Loss: 0.0745, Val Acc: 0.7800, Test Acc: 0.7850\n",
      "--- Training Complete ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Starting GNN Training ---\")\n",
    "for epoch in range(1, 201):\n",
    "    loss = train()\n",
    "    accs = test()\n",
    "\n",
    "    if epoch % 20 == 0 or epoch == 1:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
    "              f'Val Acc: {accs[\"val\"]:.4f}, Test Acc: {accs[\"test\"]:.4f}')\n",
    "\n",
    "print(\"--- Training Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rpUBYUevuHgC",
    "outputId": "f6cf0c9e-769d-4f5f-fa28-bec51be1f3a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gnn_training_performance.png generated.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = {\n",
    "    'Epoch': [1, 20, 40, 60, 80, 100, 120, 140, 160, 180, 200],\n",
    "    'Loss': [1.1000, 0.7029, 0.3602, 0.2166, 0.1287, 0.1235, 0.0989, 0.1132, 0.1023, 0.0943, 0.0745],\n",
    "    'Val Acc': [0.4880, 0.7480, 0.7680, 0.7820, 0.7760,  0.7780, 0.7760, 0.7760, 0.7840, 0.7820, 0.7800],\n",
    "    'Test Acc': [0.4530, 0.7380, 0.7580, 0.7730, 0.7770, 0.7780, 0.7740, 0.7790, 0.7820, 0.7810, 0.7850]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Training Loss (NLLLoss)', color=color)\n",
    "ax1.plot(df['Epoch'], df['Loss'], color=color, linestyle='-', marker='o', label='Loss')\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:red'\n",
    "ax2.set_ylabel('Accuracy', color=color)\n",
    "l2, = ax2.plot(df['Epoch'], df['Val Acc'], color='tab:green', linestyle='--', marker='x', label='Validation Accuracy')\n",
    "l3, = ax2.plot(df['Epoch'], df['Test Acc'], color=color, linestyle='-', marker='s', label='Test Accuracy')\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.set_ylim(0.5, 1.0) \n",
    "\n",
    "fig.suptitle('GCN Training Performance on Pubmed Dataset', fontsize=16)\n",
    "lines, labels = ax1.get_legend_handles_labels()\n",
    "lines.extend([l2, l3])\n",
    "labels.extend([l2.get_label(), l3.get_label()])\n",
    "ax1.legend(lines, labels, loc='upper center', bbox_to_anchor=(0.5, -0.08), ncol=3, frameon=False)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 0.95]) \n",
    "plt.savefig('gnn_training_performance.png')\n",
    "plt.close()\n",
    "\n",
    "print(\"gnn_training_performance.png generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8kJpMQdIwClX",
    "outputId": "e4ef3f4c-1ec7-400d-e37e-74c3161c930f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN Model initialized with: Input=1433, Hidden=16, Output=7\n"
     ]
    }
   ],
   "source": [
    "cora_dataset = Planetoid(root='./data/Cora', name='Cora')\n",
    "data = cora_dataset[0]\n",
    "\n",
    "INPUT_DIM = data.num_node_features \n",
    "HIDDEN_DIM = 16                    \n",
    "OUTPUT_DIM = cora_dataset.num_classes  \n",
    "\n",
    "model = GCN(in_channels=INPUT_DIM, hidden_channels=HIDDEN_DIM, out_channels=OUTPUT_DIM)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.NLLLoss() \n",
    "\n",
    "print(f\"GCN Model initialized with: Input={INPUT_DIM}, Hidden={HIDDEN_DIM}, Output={OUTPUT_DIM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ct4sX8R8wORv"
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    out = model(data.x, data.edge_index)\n",
    "\n",
    "    \n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "knqsIXROwR5e"
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval() \n",
    "    out = model(data.x, data.edge_index)\n",
    "\n",
    "    pred = out.argmax(dim=1)\n",
    "\n",
    "    accuracies = {}\n",
    "\n",
    "    correct_val = pred[data.val_mask] == data.y[data.val_mask]\n",
    "    accuracies['val'] = int(correct_val.sum()) / int(data.val_mask.sum())\n",
    "\n",
    "    correct_test = pred[data.test_mask] == data.y[data.test_mask]\n",
    "    accuracies['test'] = int(correct_test.sum()) / int(data.test_mask.sum())\n",
    "\n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dq6s5uV2wUrR",
    "outputId": "130aabbb-0ce7-4e79-ccff-dc476e1f9a42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Starting GNN Training For Cora\n",
      "Epoch: 001, Loss: 1.9527, Val Acc: 0.4420, Test Acc: 0.4500\n",
      "Epoch: 020, Loss: 0.2934, Val Acc: 0.7740, Test Acc: 0.8040\n",
      "Epoch: 040, Loss: 0.0805, Val Acc: 0.7700, Test Acc: 0.7810\n",
      "Epoch: 060, Loss: 0.0450, Val Acc: 0.7740, Test Acc: 0.8060\n",
      "Epoch: 080, Loss: 0.0260, Val Acc: 0.7800, Test Acc: 0.8090\n",
      "Epoch: 100, Loss: 0.0389, Val Acc: 0.7800, Test Acc: 0.8060\n",
      "Epoch: 120, Loss: 0.0342, Val Acc: 0.7620, Test Acc: 0.8000\n",
      "Epoch: 140, Loss: 0.0371, Val Acc: 0.7720, Test Acc: 0.8000\n",
      "Epoch: 160, Loss: 0.0376, Val Acc: 0.7700, Test Acc: 0.8040\n",
      "Epoch: 180, Loss: 0.0214, Val Acc: 0.7700, Test Acc: 0.8110\n",
      "Epoch: 200, Loss: 0.0365, Val Acc: 0.7660, Test Acc: 0.8070\n",
      "--- Training Complete ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Starting GNN Training For Cora\")\n",
    "for epoch in range(1, 201):\n",
    "    loss = train()\n",
    "    accs = test()\n",
    "\n",
    "    if epoch % 20 == 0 or epoch == 1:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
    "              f'Val Acc: {accs[\"val\"]:.4f}, Test Acc: {accs[\"test\"]:.4f}')\n",
    "\n",
    "print(\"--- Training Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A6HKLN_6w2Jn",
    "outputId": "30531cc3-e05b-46bf-f624-4cc5ec7f3cec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gnn_training_performance_cora.png generated.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = {\n",
    "    'Epoch': [1, 20, 40, 60, 80, 100, 120, 140, 160, 180, 200],\n",
    "    'Loss': [1.9527, 0.2934, 0.0805, 0.0450, 0.0260, 0.0389, 0.0342, 0.0371, 0.0376, 0.0214, 0.0365],\n",
    "    'Val Acc': [0.4420, 0.7740, 0.7700, 0.7740, 0.7800, 0.7800, 0.7620, 0.7720, 0.7700, 0.7700, 0.7660],\n",
    "    'Test Acc': [0.4500, 0.8040, 0.7810, 0.8060, 0.8090, 0.8060, 0.8000, 0.8000, 0.8040, 0.8110, 0.8070]\n",
    "}\n",
    "cora_df = pd.DataFrame(data)\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Training Loss (NLLLoss)', color=color)\n",
    "ax1.plot(cora_df['Epoch'], cora_df['Loss'], color=color, linestyle='-', marker='o', label='Loss')\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:red'\n",
    "ax2.set_ylabel('Accuracy', color=color)\n",
    "l2, = ax2.plot(cora_df['Epoch'], cora_df['Val Acc'], color='tab:green', linestyle='--', marker='x', label='Validation Accuracy')\n",
    "l3, = ax2.plot(cora_df['Epoch'], cora_df['Test Acc'], color=color, linestyle='-', marker='s', label='Test Accuracy')\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.set_ylim(0.5, 1.0) \n",
    "fig.suptitle('GCN Training Performance on Pubmed Dataset', fontsize=16)\n",
    "lines, labels = ax1.get_legend_handles_labels()\n",
    "lines.extend([l2, l3])\n",
    "labels.extend([l2.get_label(), l3.get_label()])\n",
    "ax1.legend(lines, labels, loc='upper center', bbox_to_anchor=(0.5, -0.08), ncol=3, frameon=False)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 0.95]) \n",
    "plt.savefig('gnn_training_performance_cora.png')\n",
    "plt.close()\n",
    "\n",
    "print(\"gnn_training_performance_cora.png generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IwJ_ItpAT5ew"
   },
   "outputs": [],
   "source": [
    "\n",
    "G_usa_mock = nx.random_geometric_graph(n=1190, radius=0.1)\n",
    "\n",
    "degree_centrality_usa = nx.degree_centrality(G_usa_mock)\n",
    "\n",
    "classes_usa = [\"High Activity\", \"Moderate Activity\", \"Moderately Low Activity\", \"Low Activity\"] # 4 classes\n",
    "sample_node_id_usa = list(G_usa_mock.nodes())[50]\n",
    "sample_text_usa = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X-3gZFoGT79H",
    "outputId": "48b198d8-9a96-4f38-e8c8-61d408a6d2d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Generated TANS Prompt Example (USA - Text-Free) ---\n",
      "Given a node from a citation network graph, where the node type is paper.\n",
      "The original node description is: \"No text found for this node.\".\n",
      "\n",
      "The following are the textual information of 5 connected nodes. The descriptions are:\n",
      "744: No text found for this node....\n",
      "518: No text found for this node....\n",
      "952: No text found for this node....\n",
      "532: No text found for this node....\n",
      "226: No text found for this node....\n",
      "\n",
      "Node Properties:\n",
      "- Degree Centrality value: 0.1749, ranked as 99.58% among all nodes.\n",
      "- Closeness Centrality value: 0.1458.\n",
      "- Betweenness Centrality value: 0.0000.\n",
      "\n",
      "Output the potential class of the node among the following classes: ['High Activity', 'Moderate Activity', 'Moderately Low Activity', 'Low Activity']. \n",
      "Provide reasons for your assessment. Your answer should be less than 200 words.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_original_text_text_free(node_id):\n",
    "    return \"\"\n",
    "\n",
    "def get_neighbor_texts_text_free(graph, node_id, num_neighbors=5):\n",
    "    return \"No textual descriptions available for connected nodes.\"\n",
    "\n",
    "\n",
    "\n",
    "high_activity_props = node_properties_df.loc[[sample_node_id_usa]].copy()\n",
    "\n",
    "high_activity_props.loc[sample_node_id_usa, \"degree_centrality\"] = 0.1749\n",
    "high_activity_props.loc[sample_node_id_usa, \"degree_centrality_rank\"] = 99.58  \n",
    "\n",
    "final_prompt_usa = generate_tans_prompt(\n",
    "    G_usa_mock,\n",
    "    sample_node_id_usa,\n",
    "    high_activity_props,\n",
    "    classes_usa\n",
    ")\n",
    "\n",
    "print(\"\\n--- Final Generated TANS Prompt Example (USA - Text-Free) ---\")\n",
    "print(final_prompt_usa)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
