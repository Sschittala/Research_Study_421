# Research Study 421: Hybrid Node Classification using GCN and TANS (Text-Attributed Node Sample) Prompting

## üìñ Overview

This repository contains the implementation and analysis scripts for a study exploring two methodologies for **node classification** in citation networks (Cora and Pubmed):

1.  Graph Neural Networks (GNN): Training a standard 2-layer Graph Convolutional Network (GCN) model on node features and graph structure.
2.  Text-Attributed Node Sample (TANS) Prompting: Employing a Large Language Model (LLM, via the Gemini API) to perform zero-shot classification by generating a structured prompt that integrates a node's textual content and its structural importance (centrality metrics).

The study aims to compare the inductive reasoning capabilities of the LLM using comprehensive metadata against the predictive performance of a structural GNN model.

---

## üíæ Datasets

This study utilizes standard datasets from the PyTorch Geometric library's `Planetoid` module:

| Dataset | Node Type | Total Nodes | Features (Dimension) | Classes |
| :--- | :--- | :--- | :--- | :--- |
| Cora | Scientific Papers | 2,708 | 1,433 | 7 |
| Pubmed | Scientific Papers | 19,717 | 500 | 3 |

---

## üõ†Ô∏è Methodology: Two Approaches

### 1. GNN Implementation (GCN)

We implement and train a standard 2-layer Graph Convolutional Network (GCN)  for both the Cora and Pubmed tasks.

* Model: `torch_geometric.nn.GCNConv` (2 layers).
* Hyperparameters:
    * Hidden Dimension: 16
    * Learning Rate (`lr`): 0.01
    * Weight Decay: $5 \times 10^{-4}$
    * Activation: ReLU
    * Regularization: Dropout (p=0.5)
* Training Objective: Node classification using Negative Log Likelihood Loss (`torch.nn.NLLLoss`).
* Evaluation: Accuracy reported on validation and test masks over 200 epochs.

### 2. TANS Prompting (LLM Classification)

The TANS method transforms the node classification problem into a **zero-shot reasoning task** for an LLM. The core input is a highly structured prompt  containing:

* Prefix & Task Definition: Defines the context (citation graph, paper type).
* Node Content: The reconstructed Bag-of-Words (BoW) pseudo-text of the target node.
* Neighbor Content: The reconstructed BoW pseudo-texts of 5 randomly sampled neighbors.
* Structural Properties (Centrality): The node's raw **Degree, Closeness, and Betweenness Centrality** scores, along with its **percentile rank** among all nodes in the graph.
* Suffix & Output Constraint: Lists the candidate classes and requires a justification in less than 200 words.

LLM Model Used: `gemini-2.5-flash`

---

## üìä Results Summary

### GCN Performance

The GCN model demonstrates strong structural prediction capabilities on both datasets:

| Dataset | Input Dim | Output Dim | Final Loss (Epoch 200) | Val Accuracy | Test Accuracy |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **Pubmed** | 500 | 3 | 0.0745 | 0.7800 | **0.7850** |
| **Cora** | 1433 | 7 | 0.0365 | 0.7660 | **0.8070** |

*Note: Training logs and performance plots are generated by the notebook (e.g., `gnn_training_performance.png`, `gnn_training_performance_cora.png`).*

### TANS Qualitative Analysis (Cora Example)

The LLM (Gemini-2.5-Flash) successfully uses the TANS prompt to generate a reasoned classification.

| Prompt Feature | Value (Sample Node 128) | LLM Reasoning Focus | Predicted Class |
| :--- | :--- | :--- | :--- |
| **Original Text** | `w2 w42 w188 ... w1394` | High co-occurrence of shared technical vocabulary. | **Neural Networks** |
| **Neighbor Texts** | 5 neighbor BoW lists (Shared keywords: `w1264`, `w582`, `w875`, etc.) | Focused research domain with distinct, consistent keywords. | |
| **Degree Rank** | 67.06% (Moderate) | Node is well-connected within its community. | |
| **Betweenness** | 0.0007 (Low) | Node is not bridging disparate areas, suggesting specialization. | |

The LLM's explanation demonstrates an ability to connect:
1. **Textual Overlap:** High frequency of shared keywords $\rightarrow$ Specialized vocabulary.
2. **Structural Centrality:** Moderate degree, low betweenness $\rightarrow$ Embedded within a focused community.
3. **Conclusion:** Specialized focus and community embedding $\rightarrow$ Classification as **Neural Networks**.

---

## üèÉ Getting Started

### Prerequisites

* Python 3.10+
* The following packages (installed in Cell 1):
    ```bash
    pip install torch torch-scatter torch-geometric networkx pandas numpy
    pip install google-genai
    ```

### Execution

1.  **Clone the repository.**
2.  **Update API Key:** Replace `'Your API'` with your actual Gemini API key in the TANS execution cell (Cell 104).
3.  **Run the Jupyter Notebook** (`Research_Study_421.ipynb`) sequentially.

The notebook executes three phases:
1.  **Cora TANS Demo:** Data loading and prompt generation demonstration.
2.  **Pubmed GCN Training:** GNN model training and evaluation.
3.  **Cora GCN Training:** GNN model training and evaluation.

The final output includes GCN performance logs and a qualitative example of the TANS LLM output.